{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8edd55",
   "metadata": {},
   "source": [
    "## From biological neurons to ANNs\n",
    "\n",
    "Neurons can be abstracted to three core elements: a **cell body (soma)**, an **axon** and **dendrites**.\n",
    "\n",
    "Dendrites take in electrical input from other firing neurons. The cell body accumulates electrical potential until a **threshold** is reached and the neuron spikes an action potential, leading to an electric discharge via the axon, which will eventually reach the dendrites of other neurons.\n",
    "\n",
    "\n",
    "### Neuron and Computation\n",
    "\n",
    "How do neurons when reduced to these characteristics relate to computation? Already in 1943 [Warren McCulloch and Walter Pitts](https://link.springer.com/article/10.1007/BF02478259) studied the relation between abstract neurons and logical operators such as AND, OR and NOT. Since all of logic can be reduced to this minimal set of operators, combining neurons in a network allows to implement arbitrary logical computations based on inputs.\n",
    "\n",
    "A McCulloch-Pitts neuron outputs either 0 or 1 just as a logical operator/function would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf827c8",
   "metadata": {},
   "source": [
    "## Logical functions\n",
    "\n",
    "Let's implement an arbitrary logical function that takes three boolean truth value arguments **a**, **b**, and **c** and evaluates the following logical expression:\n",
    "\n",
    "$$ \\neg a \\rightarrow (b \\land \\neg c) $$\n",
    "\n",
    "which is also equivalent to\n",
    "\n",
    "$$ a \\lor (b \\land \\neg c) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1500bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def logical_function(arguments:tuple):\n",
    "    \n",
    "    # not(a) -> (b and not c)\n",
    "    \n",
    "    a,b,c = arguments\n",
    "    \n",
    "    if a:\n",
    "        return 1.\n",
    "    else:\n",
    "        if (b and not c):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "all_possible_arguments = \\\n",
    "            [(0,0,0),\n",
    "             (1,0,0),\n",
    "             (1,1,0),\n",
    "             (1,1,1),\n",
    "             (0,1,0),\n",
    "             (0,1,1),\n",
    "             (0,0,1)]\n",
    "\n",
    "logical_function((0,1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419cb38",
   "metadata": {},
   "source": [
    "Now we want to see that a **perceptron** can express this function.\n",
    "\n",
    "Before applying a threshold function (activation function), the output of a perceptron is given by\n",
    "\n",
    "$$ o = \\sum_i^m W_i x_i + b$$\n",
    "\n",
    "or, in matrix notation:\n",
    "\n",
    "$$ o = W x  + b$$\n",
    "\n",
    "Note that you often also see $o = x W + b$ - it all depends on whether the input vector x is a row or a column vector. \n",
    "\n",
    "On this scalar output, we apply the thresholding function to determine whether the perceptron unit is \"firing\" (it outputs 1) or not (it outputs 0). The following threshold function is known as the **heaviside-step function**:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\begin{cases}\n",
    "    1 ,& \\text{if } x > 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "To relate this back to coarse biological inspiration, we can read the weights W as analogous to the synaptic strengths of dendritic connections, and the bias as shifting the threshold.\n",
    "\n",
    "This is all we need to know in order to define a perceptron in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b84ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def heaviside_step(x):\n",
    "    \"\"\"\n",
    "    Hard binary threshold function.\n",
    "    \n",
    "    A unit fires if a threshold of 0 is surpassed.\n",
    "    \"\"\"\n",
    "    return 1. if x > 0 else 0.\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, weight_values, bias, threshold_function=heaviside_step):\n",
    "        \n",
    "        self.weights = np.array([weight_values], dtype=np.float32)\n",
    "        self.bias = bias\n",
    "        self.threshold_function = threshold_function\n",
    "    \n",
    "    def __call__(self, x:np.ndarray):\n",
    "        \n",
    "        o = self.weights @ x + self.bias # where \"@\" is matrix multiplication (or dot product)\n",
    "        \n",
    "        return self.threshold_function(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9428059",
   "metadata": {},
   "source": [
    "Can we configure the perceptron to express the logical function from above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfad6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_weights = [1,1,1] #[2, 1, -1]\n",
    "p_bias = 0\n",
    "\n",
    "perceptron = Perceptron(weight_values=p_weights, bias=p_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef714a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the perceptron is not functionally equivalent ot the logical function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_166/4250690919.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m assert np.all(\n\u001b[0m\u001b[1;32m      2\u001b[0m     [logical_function(x) == perceptron(np.array(x)) for x in all_possible_arguments]), \\\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\"the perceptron is not functionally equivalent ot the logical function\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: the perceptron is not functionally equivalent ot the logical function"
     ]
    }
   ],
   "source": [
    "assert np.all(\n",
    "    [logical_function(x) == perceptron(np.array(x)) for x in all_possible_arguments]), \\\n",
    "\"the perceptron is not functionally equivalent ot the logical function\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb106c",
   "metadata": {},
   "source": [
    "## The logic of (binary) Multilayer Perceptrons (MLP)\n",
    "\n",
    "What about the following logical function?\n",
    "\n",
    "$$ (a \\lor b) \\land \\neg(a \\land b)$$\n",
    "\n",
    "This is called the XOR operator for \"exclusive or\".\n",
    "\n",
    "It turns out that a single perceptron can not express XOR since it is not linearly separable. Perceptrons can only express a linear function.\n",
    "\n",
    "The **linear** separatrix (decision boundary) of a perceptron is given by\n",
    "$$ w_1x_1+w_2x_2+...+w_nx_n+b = 0$$\n",
    "\n",
    "\n",
    "![https://929687.smushcdn.com/2633864/wp-content/uploads/2021/04/bitwise_datasets-1024x365.png?lossy=1&strip=1&webp=1](https://929687.smushcdn.com/2633864/wp-content/uploads/2021/04/bitwise_datasets-1024x365.png?lossy=1&strip=1&webp=1)\n",
    "\n",
    "To express this non-linear function, we can stack perceptrons on top of each other, where one perceptron can take the outputs of other perceptrons as its input arguments. This is exactly equivalent to how we can combine logical operators to create composite logical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc5d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(x):\n",
    "    a,b = x\n",
    "    return (a or b) and (not (a and b))\n",
    "\n",
    "all_possible_arguments = [\n",
    "    (0,0),\n",
    "    (0,1),\n",
    "    (1,0),\n",
    "    (1,1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58855dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a or b\n",
    "p1_weights = [1,1]\n",
    "p1_bias = 0\n",
    "\n",
    "# a and b\n",
    "p2_weights = [0.5,0.5]\n",
    "p2_bias = -0.5\n",
    "\n",
    "# p1 and not p2\n",
    "p3_weights = [0.5,-0.5]\n",
    "p3_bias = 0\n",
    "\n",
    "perceptron_1 = Perceptron(p1_weights, p1_bias)\n",
    "perceptron_2 = Perceptron(p2_weights, p2_bias)\n",
    "perceptron_3 = Perceptron(p3_weights, p3_bias)\n",
    "\n",
    "def wire_perceptrons(x):\n",
    "    \"\"\"\n",
    "    perceptron 1 and 2 take (a,b) as inputs.\n",
    "    perceptron 3 takes the outputs of perceptron 1 and perceptron 2 as inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    return perceptron_3([perceptron_1(x),perceptron_2(x)])\n",
    "\n",
    "# evaluate that our simple handwired network implements xor\n",
    "np.all([xor(x) == wire_perceptrons(np.array(x)) for x in all_possible_arguments])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07cdc56",
   "metadata": {},
   "source": [
    "We just implemented an artificial neural network that implements a non-linear logical function by setting all the weights by hand.\n",
    "\n",
    "What about other kinds of functions, like non-binary functions? Currently our perceptron can only output 1 or 0..\n",
    "\n",
    "We need to use a continuous valued threshold function. One class of such activation functions is logistic functions.\n",
    "\n",
    "## Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- saturates towards 0 and 1\n",
    "- can be read as a probability\n",
    "\n",
    "## Tanh activation function\n",
    "\n",
    "$$ \\sigma(x) = \\frac{(e^x – e^{-x})}{(e^x + e^{-x})} $$\n",
    "\n",
    "- saturates at -1 and 1\n",
    "\n",
    "\n",
    "## No activation function (sometimes referred to as linear activation)\n",
    "\n",
    "$$ \\sigma(x) = x $$\n",
    "\n",
    "- can lead to arbitrary output values depending on what the pre-activation is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a0d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def linear(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d0132a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23147522])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron(weight_values=[0.5, -0.6, 0.1], bias=0, threshold_function=sigmoid)\n",
    "\n",
    "perceptron(np.array([-3,0,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d7054",
   "metadata": {},
   "source": [
    "## (Next week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e088303d",
   "metadata": {},
   "source": [
    "## Layers of perceptrons\n",
    " \n",
    "In the XOR example we had two perceptrons that took the raw values (a,b) as inputs. These can be considered to constitute a **layer** of two perceptrons. For readability and for matrix multiplication efficiency we want to implement a layer of perceptrons as a single object instead of having an object for each unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ef2eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronLayer(object):\n",
    "    def __init__(self, weights, biases, activation_function=sigmoid):\n",
    "        \n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(biases)\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \n",
    "        output = (self.weights @ x + self.bias)\n",
    "        \n",
    "        return self.activation_function(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2af75c",
   "metadata": {},
   "source": [
    "Looks the exact same as before, right??\n",
    "\n",
    "This time, we pass a different shape of the weights array, turning the dot product into a matrix-vector multiplication.\n",
    "\n",
    "Let's instantiate a layer of 4 perceptrons that each connect to 3 input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fee40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights have shape (4, 3)\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([[1,3,4],\n",
    "                    [-2,-4,1],\n",
    "                    [3,1,9],\n",
    "                    [0,1,0]])\n",
    "\n",
    "biases = np.array([0,0,0,0])\n",
    "\n",
    "print(f\"weights have shape {weights.shape}\")\n",
    "\n",
    "perceptron_layer = PerceptronLayer(weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perceptron_layer([0.4, 1.2, 3.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f81ce",
   "metadata": {},
   "source": [
    "From the layer with 4 units we get a 4 dimensional vector instead of a single scalar as the output. \n",
    "\n",
    "How can we stack these layers and make sure that everything is going to work?\n",
    "\n",
    "All we need to do is to make sure we obey the rules of matrix multiplication!\n",
    "\n",
    "Matrix multiplication only works if the inner dimensions of the two matrices match.\n",
    "\n",
    "Given a matrix **W** (the weights of our layer) and a vector **x** (the inputs to the layer), we need the dimensionality of **x** to match the number of columns in the matrix. The output then is a vector with a dimensionality of the number of rows of **W**. This can be flipped if we use $o = x W + b$ instead or we use $o = W^T x + b$. In the latter case we can read the weights shape as (n inputs, n outputs) which may be more intuitive. Here we assume the arguably more unintuitive version in which the weights have shape (n outputs, n inputs).\n",
    "\n",
    "If we have data x with shape (10,) and a first layer with a weight shape of (5,10), we get a vector of shape (5,) as the output. If we want to apply another layer to this output, its weights need to be of shape (n_units, 5).\n",
    "\n",
    "Let's define such a network, this time using randomly initialized weights so we can also scale it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0af87d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights(*shape):\n",
    "    return np.random.normal(size=shape)\n",
    "\n",
    "class MultiLayerPerceptron(object):\n",
    "    \n",
    "    def __init__(self, n_inputs):\n",
    "        \n",
    "        self.layer_1 = PerceptronLayer(weights=random_weights(64, n_inputs),\n",
    "                                       biases=np.zeros(64))\n",
    "        \n",
    "        self.layer_2 = PerceptronLayer(weights=random_weights(32, 64),\n",
    "                                       biases=np.zeros(32))\n",
    "        \n",
    "        self.layer_3 = PerceptronLayer(weights=random_weights(16, 32),\n",
    "                                       biases=np.zeros(16))\n",
    "        \n",
    "        self.output_layer = PerceptronLayer(weights=random_weights(1, 16),\n",
    "                                            biases=np.zeros(1),\n",
    "                                            activation_function=linear)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7081083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.44910641]\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(n_inputs=3)\n",
    "\n",
    "observation = [1, -1, 0.5]\n",
    "prediction = mlp(observation)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4a60d",
   "metadata": {},
   "source": [
    "Now suppose we have an entire set of observations and we know the true values for what we want to predict.\n",
    "\n",
    "**How can we adapt the weights (and the bias values) of our multi-layer perceptron to match the predictions to the ground truth given the observations?**\n",
    "\n",
    "We will discuss how artificial neural networks can be optimized in next week's flipped classroom!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
