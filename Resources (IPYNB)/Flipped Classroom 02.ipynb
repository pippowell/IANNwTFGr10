{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae82ea0",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "In this notebook we have a look at how to implement backpropagation using (batched) matrix multiplication. We will see how activations that are computed in the forward computation of a neural network (or any other function!) can be re-used when computing gradients of the function with respect to its parameters/weights.\n",
    "\n",
    "To do so, we create a small artificial dataset of a 1D sine function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077a194",
   "metadata": {},
   "source": [
    "### (OPTIONAL) Historical background: Learning rule of a single perceptron (layer)\n",
    "\n",
    "A single perceptron (or a layer of perceptrons for multivariate targets) can be trained with a simple learning rule, the **Delta rule** that Frank Rosenblatt came up with already in 1958:\n",
    "$$\\Delta w_k^i = \\eta (y^i- \\hat{y}^i) x_k$$\n",
    "\n",
    "where $\\eta$ is a **learning rate**.\n",
    "\n",
    "Since a single layer of perceptrons can only discriminate between linearly separable input features, there is a need to use multiple perceptron layers stacked on top of each other. This limitation of perceptrons, and the lack of efficient weight update schemes for multi-layer perceptrons (and Rosenblatt's death before he could write a reply to criticism from symbolic AI proponent Marvin Minsky) effectively led to a first winter in ANN research. But now how do we update the weights in an MLP?\n",
    "\n",
    "To know how to update a weight, we need to know its contribution to the output of the MLP. How to do this efficiently was eventually figured out when David Rumelhart and Geoffrey Hinton came up with the idea of **error backpropagation** in 1985 while working on a research program they called _\"Parallel Distributed Processing\"_.\n",
    "\n",
    "The idea of backpropagation is to compute the gradients of the loss with respect to the weights. In the single layer case this could be done by the delta rule, so what the backpropagation represents is a **generalization of the delta rule** to the case where we have multiple layers of perceptrons.\n",
    "\n",
    "Later on, the idea of backpropagation was further generalized to yield algorithms for **automatic differentiation** on any function for which we know the derivatives of the individual local computations involved.\n",
    "\n",
    "Before diving into the idea of automatic differentiation, the aim of which is to have an efficient way of computing the gradients of the loss function with respect to the weights, we need to understand what _gradients of the loss function_ actually means and why finding these gradients would be useful for optimizing neural networks.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The idea of gradient descent is that we can find minima of a function by following the opposite of its **gradient**. The gradient of a function of a vector of weights $\\vec{w}$ can be written as $\\nabla_{\\vec{w}} f(\\vec{w})$ and it is a vector of the partial derivatives of $f$ with respect to each element $w_i$ in $\\vec{w}$:\n",
    "\n",
    "$$\\nabla_{\\vec{w}} f(\\vec{w}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial w_0} \\\\\\frac{\\partial f}{\\partial w_1} \\\\... \\\\ \\frac{\\partial f}{\\partial w_n}\\end{pmatrix}$$\n",
    "\n",
    "This gradient points in a linearized way into the **locally steepest direction of function value increase**. If we want to minimize an error function, the idea is to follow the opposite direction of this gradient. By iteratively following and re-computing the gradient in **small steps**, we can end up reducing the error function. Given that $f$ is convex (meaning there exist no local minima that are not identical to the global minimum) and we take small enough steps, convergence to the global minimum is guaranteed.\n",
    "\n",
    "## Back to Backpropagation\n",
    "\n",
    "We are interested in finding the partial derivatives of a loss/error function with respect to the weights. From calculus we know a bunch of rules that govern differentiation. Of particular interest to us is the **chain rule of calculus**: \n",
    "\n",
    "If we have a composite function $f(x) = g(h(x))$, then $$f'(x) = g'(h(x)) h'(x)$$\n",
    "\n",
    "Now with an MLP we do have such a composite function in the sense that one layer computes an output which then is the input to the next layer. In fact with two layers, we could define one layer as $g$ and the other layer as $h$ and yet another function $\\mathcal{L}$ as the error or **loss** function. Finding the gradients $\\nabla_w \\mathcal{L(w)}$ can be decomposed into finding partial derivatives of the layers $g$ and $h$. \n",
    "\n",
    "The idea is that we can first compute the partial derivatives of the outermost functions/layers and then, to obtain the derivatives of the earlier layers, we can multiply them with what we already computed for the outer functions. We have  $f = \\mathcal{L}(g(w_1, h(w_0, x)))$ and we want $f_w1'$, we need to first differentiate through the loss function. \n",
    "\n",
    "For the mean squared error $\\mathcal{L}_{\\text{MSE}} = \\frac{1}{2} (\\hat{y}_w - y)^2$ this equals $\\hat{y}_w - y$. To then obtain the gradients w.r.t the weights $w_1$ of the outermost layer $g$, we multiply $\\hat{y}_w - y$ with $g'_w(x)$. The same logic applies to the weights of layer $h$. Once we have the derivatives of the next layer's weights, we can compute the derivatives for the previous layer's weights etc.. It is for this logic that comes from applying the chain rule, that it was called backpropagation.\n",
    "\n",
    "As it turns out, to compute the gradients in this fashion, it is highly useful to keep track of intermediate results during the **forward pass** (calling the MLP on an input). The general logic of training MLPs thus is to first compute a prediction with a model and keep intermediate outputs in memory before using these values in the backward pass.\n",
    "\n",
    "## Automatic Differentiation\n",
    "\n",
    "While the original backpropagation learning rule was special to computations occuring in multi-layer perceptrons, a more modern approach is to have a library that tracks the **computational graph** and for each basic operation knows the derivative. During the backward pass the computational graph is reversely followed, making use of our knowledge of the derivatives of the individual small computations. With an automatic differentiation framework such as Tensorflow we can compute gradients (and thus perform gradient descent based optimization) on arbitrary functions that do not even have to be considered neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b147eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 15:59:33.055713: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 15:59:33.378791: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-03 15:59:33.378818: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-03 15:59:33.419776: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-03 15:59:34.456781: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-03 15:59:34.456857: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-03 15:59:34.456864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/mp/anaconda3/envs/uni/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58d8f8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 15:59:37.215188: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-03 15:59:37.215465: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-03 15:59:37.215491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-ATOP1V9): /proc/driver/nvidia/version does not exist\n",
      "2022-11-03 15:59:37.217426: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# this is the non-linear function we want to learn\n",
    "x = np.linspace(0,10, num=128, dtype=np.float32)\n",
    "x = x.reshape(x.shape[0], 1)\n",
    "y = np.sin(x) * 0.5 + 0.5 # sine between 0 and 1\n",
    "\n",
    "# show training data\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "# create data, a single example for simplicity\n",
    "batch_size=1\n",
    "\n",
    "inputs = np.random.uniform(low=0.0, high=10,\n",
    "                           size=batch_size).astype(np.float32)\n",
    "inputs = inputs.reshape(batch_size, 1)\n",
    "target = np.sin(inputs)*0.5 + 0.5\n",
    "\n",
    "# turn numpy arrays into tensorflow tensors\n",
    "inputs = tf.constant(inputs)\n",
    "target = tf.constant(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f70bb1",
   "metadata": {},
   "source": [
    "Next we create a neural network (multi-layer perceptron) that takes this data and transforms it, using linear transformations (matrix multiplication) and non-linear activation functions (sigmoid), to compute predictions of the sine value.\n",
    "\n",
    "We will use 3 layers. The first layer has 20 hidden units, the second layer has 10 hidden units and the third layer, which is the output layer, has 1 unit (just like the target). We use tensorflow instead of numpy but really we use it in pretty much the same way as numpy.\n",
    "\n",
    "The weight matrices have shape (1,20), (20,10) and (10,1), following the convention (n_in, n_out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1de0f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = tf.random.uniform(shape=(1,20))\n",
    "weights_2 = tf.random.uniform(shape=(20,10))\n",
    "weights_3 = tf.random.uniform(shape=(10,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84155702",
   "metadata": {},
   "source": [
    "### The forward computation of an ANN\n",
    "\n",
    "Next we define the computations in which these weights are used.\n",
    "\n",
    "We have 3 layers and we want to apply a sigmoid activation function after every layer.\n",
    "\n",
    "We also compute the error between the output and the target values using a **Mean Squared Error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e617c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions are 0.9623635411262512 \n",
      " while the targets are 0.8839762210845947\n",
      "\n",
      " This makes a mean squared error of 0.0030722860246896744\n"
     ]
    }
   ],
   "source": [
    "# layer 1\n",
    "f7_o = inputs @ weights_1\n",
    "# activation function of layer 1\n",
    "f6_o = tf.nn.sigmoid(f7_o)\n",
    "\n",
    "# layer 2\n",
    "f5_o = f6_o @ weights_2\n",
    "# activation function of layer 2\n",
    "f4_o = tf.nn.sigmoid(f5_o)\n",
    "\n",
    "# layer 3 (output layer)\n",
    "f3_o = f4_o @ weights_3\n",
    "# activation function of layer 3 (output layer)\n",
    "f2_o = tf.nn.sigmoid(f3_o)\n",
    "\n",
    "# error function, here MSE\n",
    "f1_o = tf.reduce_mean(0.5 * (f2_o - target)**2, axis=None) # mean squared error\n",
    "\n",
    "print(f\"predictions are {f2_o.numpy().squeeze()} \\n while the targets are {target.numpy().squeeze()}\")\n",
    "print(f\"\\n This makes a mean squared error of {f1_o}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d3f10",
   "metadata": {},
   "source": [
    "Our model can be described as a composite function of 7 computations, most of which are already composite functions (e.g. sigmoid activation function or the MSE-loss):\n",
    "\n",
    "$$\\mathcal{L} = f_1(f_2(f_3( f_4(f_5(f_6(f_7(x,w_1)), w_2  )  ) ) , w_3)   , \\text{target})$$\n",
    "\n",
    "What we want to find is the tuple of partial derivatives, that is the gradient, of $f_1$ w.r.t. the parameters ($w_1, w_2, w_3$).\n",
    "\n",
    "#### Using the chain rule\n",
    "\n",
    "Because this is a composite function we can apply the **chain rule** of calculus, according to which the derivative is computed as the outer derivative given the unchanged output of the inner function, multiplied by the derivative of the inner function, again keeping its input unchanged, if $x$ in the formula below turns out to be another function:\n",
    "\n",
    "$$\\frac{d}{{dx}}\\left[ {f_1\\left( f_2 \\right)} \\right] = \\frac{d}{{df_2}}\\left[ {f_1\\left( f_2 \\right)} \\right]\\frac{{df_2}}{{dx}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a4a73",
   "metadata": {},
   "source": [
    "### Local derivatives\n",
    "\n",
    "All we need to compute the partial derivatives of the loss function w.r.t. the network parameters (here only the weights, since we do not use a bias) is the derivatives of the individual or composite computations involved.\n",
    "\n",
    "Our composite function (the joint forward pass and loss computation of an ANN) makes use of three kinds of composite computations:\n",
    "- Mean squared error (MSE)\n",
    "- Sigmoid activation function\n",
    "- Matrix multiplication\n",
    "\n",
    "### Local derivatives of MSE\n",
    "\n",
    "$\\mathcal{L}_{\\text{MSE}} = f_1(\\text{f2_o}) = \\frac{1}{2} (\\text{f2_o} - \\text{t})^2$\n",
    "\n",
    "The derivative of this w.r.t. the ANN's final layer's output *f2_o* is\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial \\text{f2_o}} = \\text{f2_o} - \\text{t}$\n",
    "\n",
    "### Local derivatives of the sigmoid activation function\n",
    "\n",
    "Activation functions are typically denoted as $\\sigma(x)$. Here, the sigmoid activation function is defined by:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1+\\text{e}^{-x}}$\n",
    "\n",
    "and its derivative turns out to be\n",
    "\n",
    "$\\frac{\\partial \\sigma}{\\partial \\text{x}} = \\sigma(x) ( 1 - \\sigma(x))$\n",
    "\n",
    "Since an activation function is an element-wise operation, we also chain this local derivative $\\sigma'(x)$ with $\\frac{\\partial \\mathcal{L}}{\\partial \\sigma(x)}$ by element-wise multiplication.\n",
    "\n",
    "### Local derivatives of matrix multiplication\n",
    "\n",
    "A matrix multiplication is a linear operation that takes two arguments, so we can differentiate w.r.t. either the first matrix or the second matrix - in neural networks this is either the input to the layer or the weights of that layer.\n",
    "\n",
    "Let's say we have the following matrix multiplication\n",
    "\n",
    "$C = A B$\n",
    "\n",
    "and we already have the gradients of the loss function w.r.t. the output of this matrix multiplication C, that is we have $\\frac{\\partial \\mathcal{L}}{\\partial C}$.\n",
    "\n",
    "The partial derivative of the loss $\\mathcal{L}$ w.r.t. $A$ turns out to be\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial A} = \\frac{\\partial \\mathcal{L}}{\\partial C} B^{\\text{T}}$.\n",
    "\n",
    "Likewise the partial derivative w.r.t. matrix $B$ turns out to be\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial B} = A^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial C}$.\n",
    "\n",
    "**For a handwritten explanation, please refer to the additional material provided in the same folder as this notebook.**\n",
    "\n",
    "\n",
    "This is it, we know all the local derivatives involved in the computation of the loss that we want to minimize by changing the weights in the three layers. \n",
    "\n",
    "Let's have another look at the forward computation of the loss, in which we store intermediate computations and later refer to them in the math notation of the backpropagation by their variable names, hopefully making it easier to relate the math to the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef201570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction is 0.9623635411262512 while target is 0.8839762210845947\n",
      "\n",
      " This makes a mean squared error of 0.0030722860246896744\n"
     ]
    }
   ],
   "source": [
    "# layer 1\n",
    "f7_o = tf.matmul(inputs, weights_1)\n",
    "# activation function of layer 1\n",
    "f6_o = tf.nn.sigmoid(f7_o)\n",
    "\n",
    "# layer 2\n",
    "f5_o = tf.matmul(f6_o, weights_2)\n",
    "# activation function of layer 2\n",
    "f4_o = tf.nn.sigmoid(f5_o)\n",
    "\n",
    "# layer 3 (output layer)\n",
    "f3_o = tf.matmul(f4_o, weights_3)\n",
    "# activation function of layer 3 (output layer)\n",
    "f2_o = tf.nn.sigmoid(f3_o)\n",
    "\n",
    "# error function, here MSE\n",
    "f1_o = tf.reduce_mean(0.5 * (f2_o - target)**2,axis=None) # mean squared error\n",
    "\n",
    "print(f\"prediction is {f2_o.numpy().squeeze()} while target is {target.numpy().squeeze()}\")\n",
    "print(f\"\\n This makes a mean squared error of {f1_o}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5397a32e",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "We start with the partial derivative of the mean squared error w.r.t. the network output f2_o:\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_2} = \\text{f2_o} - \\text{t}$$\n",
    "\n",
    "Next we want to differentiate through the sigmoid activation function. We take an element-wise product between the outer derivative and the inner derivative $\\sigma'(x)$, the definition of which you can find in the *local derivatives* section.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_3} = \\frac{\\partial \\mathcal{L}}{\\partial f_2} \\circ \\sigma'(\\text{f3_o})$$\n",
    "\n",
    "We can already compute the gradients w.r.t. the weights of the last layer. We use the result we got above in the *local derivatives* section to know how to do this.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{W}_3} = \\text{f4_o}^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial f_3}$$\n",
    "\n",
    "Now we want to differentiate with respect to the last layer's input, f4_o - again using the logic from the *local derivatives* section.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_4} = \\frac{\\partial \\mathcal{L}}{\\partial f_3} {\\text{W}_3}^{\\text{T}}$$\n",
    "\n",
    "The next step to differentiate through backwards is again a sigmoid activation function. We can thus follow the same procedure as we did before:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_5} = \\frac{\\partial \\mathcal{L}}{\\partial f_4} \\circ \\sigma'(\\text{f5_o})$$\n",
    "\n",
    "Now we want to differentiate through the matrix multiplication of layer 2, first w.r.t. the layer's input:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_6} = \\frac{\\partial \\mathcal{L}}{\\partial f_5} {\\text{W}_3}^{\\text{T}}$$\n",
    "\n",
    "We can now get the gradients of the weights of layer 2, by differentiating the same matmul operation from the forward computation w.r.t. the weights, following the same logic as before for the gradients of weights_3:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{W}_2} = \\text{f6_o}^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial f_6}$$\n",
    "\n",
    "This is becoming quite repetitive - given that we have the gradients of the loss function w.r.t. the second layer's inputs, we can differentiate through the activation function of layer 1:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial f_7} = \\frac{\\partial \\mathcal{L}}{\\partial f_6} \\circ \\sigma'(\\text{f7_o})$$\n",
    "\n",
    "Sometimes we are interested in the gradients of the loss w.r.t. the input data. We can obtain these by differentiating through the matrix multiplication of the first layer w.r.t. the inputs:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{inputs}} = \\frac{\\partial \\mathcal{L}}{\\partial f_7}  {\\text{W}_1}^{\\text{T}}$$\n",
    "\n",
    "Finally, and lastly, we compute the gradients of the loss $\\mathcal{L}$ w.r.t. the first layer's weights:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\text{W}_1} = \\text{f7_o}^{\\text{T}} \\frac{\\partial \\mathcal{L}}{\\partial f_7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3ad30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients of MSE (d L/ d f2)\n",
    "grad_f1_o = (f2_o - target)\n",
    "\n",
    "# Chain gradients of MSE with gradients of output activation function\n",
    "# (d L / d f3)\n",
    "grad_f2_o = grad_f1_o *(tf.nn.sigmoid(f3_o)*(1-tf.nn.sigmoid(f3_o)))\n",
    "\n",
    "# compute gradients w.r.t the last layer's weights (weights_3)\n",
    "# (d L / d weights_3)\n",
    "grad_w3 = (tf.transpose(f4_o) @ grad_f2_o) / batch_size\n",
    "\n",
    "# chain with derivative of matrix multiplication of layer 3\n",
    "# d L / d f4\n",
    "grad_f3_o = grad_f2_o @ tf.transpose(weights_3)\n",
    "\n",
    "# chain with derivative of activation function (layer 3)\n",
    "# d L / d f5\n",
    "grad_f4_o = grad_f3_o * (tf.nn.sigmoid(f5_o)*(1-tf.nn.sigmoid(f5_o)))\n",
    "\n",
    "# chain with derivative of matrix multiplication of layer 2\n",
    "# d L / d f6\n",
    "grad_f5_o = grad_f4_o @ tf.transpose(weights_2)\n",
    "\n",
    "# compute gradients w.r.t. layer 2's weight matrix\n",
    "# d L / d weights_2\n",
    "grad_w2 = (tf.transpose(f6_o) @ grad_f4_o) / batch_size\n",
    "\n",
    "# chain with layer 2's sigmoid activation function\n",
    "# d L / d f7\n",
    "grad_f6_o = grad_f5_o * (tf.nn.sigmoid(f7_o)*(1-tf.nn.sigmoid(f7_o)))\n",
    "\n",
    "# compute gradients w.r.t. the input\n",
    "# d L / d inputs\n",
    "grad_inputs = grad_f6_o @ tf.transpose(weights_1)\n",
    "\n",
    "# compute gradients w.r.t. layer 1's weights\n",
    "# d L / d weights_1\n",
    "grad_w1 = (tf.transpose(inputs) @ grad_f6_o) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e77a0",
   "metadata": {},
   "source": [
    "Let's check whether our implementation that we got by chaining local derivatives of atomic computations involved actually gives us the correct gradients w.r.t. to the network weights.\n",
    "\n",
    "To do this, we rely on Tensorflow's **reverse mode automatic differentiation**, which does exactly what we just did, relying on optimized computational graph structures to reduce the number of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52b63eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([weights_1, weights_2, weights_3])\n",
    "    f7_o = tf.matmul(inputs, weights_1)\n",
    "    f6_o = tf.nn.sigmoid(f7_o)\n",
    "    f5_o = tf.matmul(f6_o, weights_2)\n",
    "    f4_o = tf.nn.sigmoid(f5_o)\n",
    "    f3_o = tf.matmul(f4_o, weights_3)\n",
    "    f2_o = tf.nn.sigmoid(f3_o)\n",
    "\n",
    "    f1_o = tf.reduce_mean(0.5 * (f2_o - target)**2) # mean squared error\n",
    "    \n",
    "real_grad_w1, real_grad_w2, real_grad_w3 = tape.gradient(f1_o, [weights_1,\n",
    "                                                                weights_2,\n",
    "                                                                weights_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59823c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients for weights_1 match tensorflow oracle:     True\n",
      "gradients for weights_2 match tensorflow oracle:     True\n",
      "gradients for weights_3 match tensorflow oracle:     True\n"
     ]
    }
   ],
   "source": [
    "tolerance = 0.000000000001\n",
    "print(f\"gradients for weights_1 match tensorflow oracle: \\\n",
    "    {np.allclose(grad_w1, real_grad_w1, atol=tolerance)}\")\n",
    "print(f\"gradients for weights_2 match tensorflow oracle: \\\n",
    "    {np.allclose(grad_w2, real_grad_w2, atol=tolerance)}\")\n",
    "print(f\"gradients for weights_3 match tensorflow oracle: \\\n",
    "    {np.allclose(grad_w3, real_grad_w3, atol=tolerance)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab1b8b",
   "metadata": {},
   "source": [
    "## Some (important) notes\n",
    "\n",
    "You may encounter some unanswered questions when trying to find gradients for more complicated composite functions. One possibility is that the same weight matrix (or activation tensor) affects the loss through multiple paths. In this case we need to aggregate the gradients obtained through these paths by summation. The same applies for a case in which we have a batch of inputs and not a single (input, output) example. In this case, you also aggregate the gradients obtained for each example by averaging, as we've done above. Deep Learning frameworks do batched matrix multiplications in a highly optimized way by utilizing the capabilities for parallelism of modern graphic processing units (GPUs) or other accelerators.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We have found the computations needed to obtain the gradients of our composite function w.r.t. some variables (here weights). Let's note some important observations that we may have made along the way:\n",
    "\n",
    "\n",
    "- Doing it by hand is tedious and the mathematical expressions for the gradients get very long, even for a shallow neural network of just 3 layers.\n",
    "\n",
    "\n",
    "- We really only need to know the local derivatives of the basic computations involved in a composite function (such as any kind of ANN) to compute gradients of the complete function.\n",
    "\n",
    "\n",
    "- The intermediate computations that arise in the function's forward pass can be re-used in the backward computation of its gradients.\n",
    "\n",
    "\n",
    "- The same logic can be applied for any kind of directed acyclic graph composed of smaller functions that are differentiable. As such, it can be used to compute gradients for any neural network architecture, as long as the individual network components have known local derivatives.\n",
    "\n",
    "\n",
    "- In automatic differentiation frameworks (esp. in tensorflow), functions are constructed on graphs that contain inputs, parameters, as well as the operations, their local derivatives and the intermediate results to be re-used in the backward pass. There is no need to chain derivatives together manually in these frameworks as we did here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fb1e3",
   "metadata": {},
   "source": [
    "## Optional: Using the hard-coded backprop to train a neural network on the sine data\n",
    "\n",
    "- we use batches of (x,y) samples from our 1D sine function for each gradient computation\n",
    "\n",
    "- we initialize the weights differently (more on this in two weeks)\n",
    "\n",
    "- we include a bias in our network\n",
    "\n",
    "- we remove the sigmoid on the output, because it is a regression task\n",
    "\n",
    "To train the network weights, we need four functions:\n",
    "- the **forward computation** (including the loss function), that also returns the intermediate results\n",
    "- the **backward (gradient) computation** that takes the results from forward computation and backpropagates the gradients to the network parameters, returning a tuple of parameter gradients.\n",
    "- the **weight update**, in which we pair the gradients with their respective parameter array/tensor and subtract a fraction of the gradients from the parameters\n",
    "- the **training loop** which applies the three functions for a number of iterations, computing the loss (and intermediate activations and pre-activations), backpropagating the error (computing gradients), and applying the gradients to update the weights and the biases of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d49c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate network parameters\n",
    "n_units_1 = 20\n",
    "n_units_2 = 10\n",
    "n_units_3 = 1\n",
    "\n",
    "# He-Uniform initialization for better learning\n",
    "limit_1 = np.sqrt(6 / 1)\n",
    "limit_2 = np.sqrt(6 / n_units_1)\n",
    "limit_3 = np.sqrt(6/n_units_2)\n",
    "\n",
    "weights_1 = tf.random.uniform(minval=-limit_1,maxval=limit_1,shape=(1,n_units_1))\n",
    "weights_2 = tf.random.uniform(minval=-limit_2,maxval=limit_2,shape=(n_units_1,n_units_2))\n",
    "weights_3 = tf.random.uniform(minval=-limit_3,maxval=limit_3, shape=(n_units_2,n_units_3))\n",
    "\n",
    "# this time using a bias in our model\n",
    "bias_1 = tf.Variable(tf.zeros(n_units_1))\n",
    "bias_2 = tf.Variable(tf.zeros(n_units_2))\n",
    "bias_3 = tf.Variable(tf.zeros(n_units_3))\n",
    "\n",
    "weights_1 = tf.Variable(weights_1)\n",
    "weights_2 = tf.Variable(weights_2)\n",
    "weights_3 = tf.Variable(weights_3)\n",
    "\n",
    "weights = (weights_1, weights_2, weights_3, bias_1, bias_2, bias_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "247813ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True) # for speed\n",
    "def compute_forward_pass(inputs, target, weights):\n",
    "    \n",
    "    weights_1 = weights[0]\n",
    "    weights_2 = weights[1]\n",
    "    weights_3 = weights[2]\n",
    "    bias_1 = weights[3]\n",
    "    bias_2 = weights[4]\n",
    "    bias_3 = weights[5]\n",
    "    \n",
    "    # layer 1, now with bias\n",
    "    f7_o = tf.matmul(inputs, weights_1) + bias_1\n",
    "    # activation function of layer 1\n",
    "    f6_o = tf.nn.sigmoid(f7_o)\n",
    "    \n",
    "    # layer 2, now with bias\n",
    "    f5_o = tf.matmul(f6_o, weights_2) + bias_2\n",
    "    # activation function of layer 2\n",
    "    f4_o = tf.nn.sigmoid(f5_o)\n",
    "\n",
    "    # layer 3 (output layer), now with bias\n",
    "    f3_o = tf.matmul(f4_o, weights_3) + bias_3\n",
    "    # activation function of layer 3 (output layer)\n",
    "    # skip the last layer's sigmoid f2_o = tf.nn.sigmoid(f3_o) \n",
    "\n",
    "    # error function, here MSE\n",
    "    f1_o = tf.reduce_mean(0.5 * (f3_o - target)**2,axis=None) # mean squared error\n",
    "\n",
    "    return (f1_o, f3_o, f4_o, f5_o, f6_o, f7_o, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb360018",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(jit_compile=True) # for speed\n",
    "def compute_gradients(weights, data_from_forward, target, batch_size):\n",
    "    \n",
    "    # unpack weights and data from the forward-step\n",
    "    (weights_1, weights_2, weights_3, bias_1, bias_2, bias_3) = weights\n",
    "    (f1_o, f3_o, f4_o, f5_o, f6_o, f7_o, inputs) = data_from_forward\n",
    "    \n",
    "    # Gradients of MSE (d L/ d f2)\n",
    "    grad_f1_o = (f3_o - target)\n",
    "    \n",
    "    # we skipped the output layer's sigmoid\n",
    "\n",
    "    # chain with derivative of matrix multiplication of layer 3\n",
    "    # d L / d f4\n",
    "    grad_f3_o = grad_f1_o @ tf.transpose(weights_3)\n",
    "    \n",
    "    # compute gradients w.r.t the last layer's weights (weights_3)\n",
    "    # (d L / d weights_3)\n",
    "    grad_w3 = (tf.transpose(f4_o) @ grad_f1_o) / batch_size\n",
    "    \n",
    "    # (d L / d bias_3)\n",
    "    grad_b3 = tf.reduce_sum(grad_f1_o,axis=0) / batch_size\n",
    "\n",
    "    # chain with derivative of activation function (layer 3)\n",
    "    # d L / d f5\n",
    "    grad_f4_o = grad_f3_o * (tf.nn.sigmoid(f5_o)*(1-tf.nn.sigmoid(f5_o)))\n",
    "\n",
    "    # chain with derivative of matrix multiplication of layer 2\n",
    "    # d L / d f6\n",
    "    grad_f5_o = grad_f4_o @ tf.transpose(weights_2)\n",
    "\n",
    "    # compute gradients w.r.t. layer 2's weight matrix\n",
    "    # d L / d weights_2\n",
    "    grad_w2 = (tf.transpose(f6_o) @ grad_f4_o) / batch_size\n",
    "    \n",
    "    # (d L / d bias_2)\n",
    "    grad_b2 = tf.reduce_sum(grad_f4_o,axis=0) / batch_size\n",
    "\n",
    "    # chain with layer 2's sigmoid activation function\n",
    "    # d L / d f7\n",
    "    grad_f6_o = grad_f5_o * (tf.nn.sigmoid(f7_o)*(1-tf.nn.sigmoid(f7_o)))\n",
    "\n",
    "    # compute gradients w.r.t. the input\n",
    "    # d L / d inputs\n",
    "    grad_inputs = grad_f6_o @ tf.transpose(weights_1)\n",
    "\n",
    "    # compute gradients w.r.t. layer 1's weights\n",
    "    # d L / d weights_1\n",
    "    grad_w1 = (tf.transpose(inputs) @ grad_f6_o) / batch_size\n",
    "    \n",
    "    # (d L / d bias_1)\n",
    "    grad_b1 = tf.reduce_sum(grad_f6_o,axis=0) / batch_size\n",
    "\n",
    "    return (grad_w1, grad_w2, grad_w3, grad_b1, grad_b2, grad_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c977ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, weight_gradients, learning_rate=1e-2):\n",
    "    for w, w_grad in zip(weights, weight_gradients):\n",
    "        w = w.assign(w - learning_rate*w_grad)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51cde2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_loss(weights, batch_size, steps, \n",
    "                  learning_rate):\n",
    "    # random subset of data\n",
    "    losses = []\n",
    "    for i in range(steps):\n",
    "        \n",
    "        # create training data on the fly\n",
    "        inputs = np.random.uniform(low=0.0, high=10,size=batch_size).astype(np.float32)\n",
    "        inputs = inputs.reshape(batch_size, 1)\n",
    "        target = np.sin(inputs)*0.5 + 0.5\n",
    "\n",
    "        # turn numpy arrays into tensorflow tensors\n",
    "        random_inputs = tf.constant(inputs)\n",
    "        random_target = tf.constant(target)\n",
    "        \n",
    "        data_from_forward = compute_forward_pass(random_inputs, \n",
    "                                                 random_target, \n",
    "                                                 weights)\n",
    "        # report loss every 100 epochs\n",
    "        losses.append(data_from_forward[0])\n",
    "        if not i%8000: \n",
    "            print(f\"loss is {np.mean(losses)}\")\n",
    "            losses = []\n",
    "\n",
    "        grads = compute_gradients(weights, data_from_forward, \n",
    "                                  random_target, \n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "        weights = update_weights(weights,grads,learning_rate=learning_rate)\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a610eb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 15:12:52.839318: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x555790ddc5f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-02 15:12:52.839357: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Host, Default Version\n",
      "2022-11-02 15:12:52.869156: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-02 15:12:53.050489: I tensorflow/compiler/jit/xla_compilation_cache.cc:476] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 1.367553472518921\n",
      "loss is 0.05443808063864708\n",
      "loss is 0.050000909715890884\n",
      "loss is 0.04771801084280014\n",
      "loss is 0.04488663375377655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_130/4099368996.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trained_weights = minimize_loss(weights=weights,\n\u001b[0m\u001b[1;32m      2\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       learning_rate=0.01)\n",
      "\u001b[0;32m/tmp/ipykernel_130/1778138110.py\u001b[0m in \u001b[0;36mminimize_loss\u001b[0;34m(weights, batch_size, steps, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         grads = compute_gradients(weights, data_from_forward, \n\u001b[0m\u001b[1;32m     26\u001b[0m                                   \u001b[0mrandom_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                   batch_size=batch_size)\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m-> 2495\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2496\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2723\u001b[0m     \u001b[0;31m# only active captures should be saved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m       func_cache_key, _ = function_context.make_cache_key(\n\u001b[0m\u001b[1;32m   2726\u001b[0m           (args, kwargs), captures)\n\u001b[1;32m   2727\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/python/eager/function_context.py\u001b[0m in \u001b[0;36mmake_cache_key\u001b[0;34m(args, captures)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mcaptures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0msignature_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalTracingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m   args_signature = trace_type.from_object(\n\u001b[0m\u001b[1;32m    132\u001b[0m       args, signature_context)\n\u001b[1;32m    133\u001b[0m   captures_dict_tracetype = trace_type.from_object(\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_object\u001b[0;34m(obj, context)\u001b[0m\n\u001b[1;32m    109\u001b[0m           named_tuple_type, tuple(from_object(c, context) for c in obj))\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m           named_tuple_type, tuple(from_object(c, context) for c in obj))\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_object\u001b[0;34m(obj, context)\u001b[0m\n\u001b[1;32m    109\u001b[0m           named_tuple_type, tuple(from_object(c, context) for c in obj))\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    109\u001b[0m           named_tuple_type, tuple(from_object(c, context) for c in obj))\n\u001b[1;32m    110\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_object\u001b[0;34m(obj, context)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInternalTracingContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsTracingProtocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_tracing_type__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/typing.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;31m# assigned in __init__.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         if ((not getattr(cls, '_is_protocol', False) or\n\u001b[0;32m-> 1114\u001b[0;31m                 _is_callable_members_only(cls)) and\n\u001b[0m\u001b[1;32m   1115\u001b[0m                 issubclass(instance.__class__, cls)):\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/typing.py\u001b[0m in \u001b[0;36m_is_callable_members_only\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_callable_members_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0;31m# PEP 544 prohibits using issubclass() with protocols that have non-method members.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_get_protocol_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/uni/lib/python3.9/typing.py\u001b[0m in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__annotations__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_abc_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEXCLUDED_ATTRIBUTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m                 \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_weights = minimize_loss(weights=weights,\n",
    "                      batch_size=128, steps=300000,\n",
    "                      learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39dcf453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data\n",
    "x = np.linspace(0,10,1000).astype(np.float32)\n",
    "x = x.reshape(1000,1)\n",
    "y = np.sin(x)*0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd4a18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = compute_forward_pass(x, y, weights)[1]\n",
    "plt.plot(x, prediction.numpy())\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
