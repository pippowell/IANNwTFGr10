{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prizl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# 2.1 Load Dataset\n",
    "(train_ds, test_ds), ds_info = tfds.load ('mnist', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "# print(\"ds_info: \\n\", ds_info)\n",
    "# tfds.show_examples(train_ds, ds_info)\n",
    "\n",
    "# 2.2 Data Pipeline\n",
    "def preprocess(dataset, batchsize):\n",
    "\n",
    "    '''\n",
    "    :param dataset: the dataset to be prepared for input into the network\n",
    "    :param batchsize: the desired batchsize\n",
    "    :return: 2 datasets, one each for each of the math problems defined (see below), created after the original database was preprocessed with the\n",
    "    steps below\n",
    "    '''\n",
    "\n",
    "    # Step 1 - General Preprocessing\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors\n",
    "    dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "    \n",
    "    # Step 2 - Pairing Data Tuples & Respective Parameterized Targets\n",
    "\n",
    "    # create a dataset that contains 2000 samples from the overall dataset paired with 2000 other samples\n",
    "    data = tf.data.Dataset.zip((dataset.shuffle(2000), dataset.shuffle(2000), dataset.shuffle(2000), dataset.shuffle(2000)))\n",
    "    \n",
    "    # create the dataset for the first math problem (a + b >= 5) - remembering to cast to int versus boolean!\n",
    "    first = data.map(lambda x1, x2, x3, x4: (x1[0], x2[0], x3[0], x4[0], x1[1]))\n",
    "    second = data.map(lambda x1, x2, x3, x4: (x1[0], x2[0], x3[0], x4[0], x1[1] - x2[1]))\n",
    "    third = data.map(lambda x1, x2, x3, x4: (x1[0], x2[0], x3[0], x4[0], x1[1] - x2[1] + x3[1]))\n",
    "    fourth = data.map(lambda x1, x2, x3, x4: (x1[0], x2[0], x3[0], x4[0], x1[1] - x2[1] + x3[1] - x4[1]))\n",
    "    \n",
    "    list = [first, second, third, fourth]\n",
    "\n",
    "    # Step 3 - Batching & Prefetching\n",
    "    for i in list: \n",
    "        i = i.batch(batchsize)\n",
    "        i = i.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # return BOTH datasets\n",
    "    return first, second, third, fourth\n",
    "\n",
    "train_dataset = preprocess(train_ds, 32)\n",
    "test_dataset = preprocess(test_ds, 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.1 Finding the targets (instruction still in revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The CNN & LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(tf.keras.Model):\n",
    "    def __init__(self, batch, sequence_length, image):\n",
    "        super(BasicConv, self).__init__()\n",
    "\n",
    "        # input 32x32x3 with 3 as the color channels\n",
    "        self.convlayer1 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # after this: 32x32x24\n",
    "        # self.convlayer2 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "        # self.convlayer3 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "\n",
    "        # self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2) # 16x16x24\n",
    "\n",
    "        # self.normlayer = tf.keras.layers.Normalization(axis=-1,mean=None,invert=False)\n",
    "\n",
    "        # self.convlayer4 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x72\n",
    "        # self.convlayer5 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x72\n",
    "        # self.convlayer6 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x72\n",
    "\n",
    "        self.global_pool = tf.keras.layers.GlobalAvgPool2D() # 1x1x72\n",
    "\n",
    "        # self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        self.metrics_list = [\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"acc\"), # only for subtask 0, not for subtask 1\n",
    "                    ]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.convlayer1(x) ## trying it out as simple as possible\n",
    "        # x = self.convlayer2(x)\n",
    "        # x = self.convlayer3(x)\n",
    "        # x = self.pooling(x)\n",
    "        # x = self.convlayer4(x)\n",
    "        # x = self.convlayer5(x)\n",
    "        # x = self.convlayer6(x)\n",
    "        # x = self.global_pool(x)\n",
    "        x = tf.keras.layers.TimeDistributed(self.global_pool())(x)\n",
    "\n",
    "        # Once you have encoded all images as vectors, the shape of the tensor should be (batch, sequence-length, features), \n",
    "        # which can be fed to a non-convolutional standard LSTM.\n",
    "        return x\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input):\n",
    "        img, label = input\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(img, training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "\n",
    "        img, label = input\n",
    "\n",
    "        prediction = self(img, training=False)\n",
    "        loss = self.loss_function(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction)\n",
    "\n",
    "        # return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 LSTM AbstractRNNCell layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
