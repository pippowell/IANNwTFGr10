*Note that the following questions were answered in comparison to the
performance of the network after 10 epochs feeding all data points per
epoch, to reduce run time during these experimental runs.*

Changing the Function:
The network manages to determine each of the functions with fairly high accuracy, though it performs best for cosine and has the hardest time with the chaotic function
(although even with that function, it still performs quite well).

Changing the Dataset Size:
As expected, a larger dataset size results in a lower final loss, i.e. the network learns the function better when fed more data.

Changing # Perceptrons in Hidden Layer:
Curiously, the network actually performs slightly worse with higher numbers of perceptrons in the hidden layer, 
although increasing from 10 to 15 does result in a slight decrease in final loss. 
Perhaps here, adding more perceptrons is resulting in a model that is too complex for the data 
and therefore is running into an overfitting-style problem?

Using Evenly Spaced Data:
With evenly spaced data, the network curiously temporarily moves further away from 
an accurate representation of the input function before honing in on it in later epochs.
Perhaps here, the model is picking up on the even spacing and following this lead versus the actual function, 
therefore causing a delay before it starts narrowing in on the correct formula?
