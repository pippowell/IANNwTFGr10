{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IANNwTF HW 4\n",
    "## Group 10\n",
    "\n",
    "The following contains our solution to the exercises in IANNwTF HW 04. A Jupyter notebook versus a module format was chosen this time for purposes of organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assigment 1: Reviews\n",
    "We review the homeworks for Groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 2: MNIST Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Preparing the MNIST Math Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Needed Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Load Dataset\n",
    "(train_ds, test_ds), ds_info = tfds.load ('mnist', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "# Info on the dataset (refresher)\n",
    "# print(\"ds_info: \\n\", ds_info)\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.2 Data Pipeline\n",
    "def prepare_data(dataset, batchsize):\n",
    "\n",
    "    '''\n",
    "    :param dataset: the dataset to be prepared for input into the network\n",
    "    :return: 2 datasets, one each for each of the math problems defined (see below), created after the original database was preprocessed with the\n",
    "    steps below\n",
    "    '''\n",
    "\n",
    "    # Step One - General Preprocessing\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors\n",
    "    dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # Step 2 - Pairing Data Tuples & Respective Parameterized Targets\n",
    "\n",
    "    # create a dataset that contains 2000 samples from the overall dataset paired with 2000 other samples\n",
    "    data = tf.data.Dataset.zip((dataset.shuffle(2000), dataset.shuffle(2000)))\n",
    "\n",
    "    # create the dataset for the first math problem (a + b >= 5) - remembering to cast to int versus boolean!\n",
    "    greateqfive = data.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5))\n",
    "    greateqfive = greateqfive.map(lambda x1, x2, t: (x1, x2, tf.cast(t, tf.int32)))\n",
    "\n",
    "    # create the dataset for the second math problem (a - b = y)\n",
    "    subtr = data.map(lambda x1, x2: (x1[0], x2[0], x1[1]-x2[1]))\n",
    "\n",
    "    # Step 3 - Batching & Prefetching\n",
    "\n",
    "    # run batching and prefetching for both datasets\n",
    "    greateqfive = greateqfive.batch(batchsize)\n",
    "    greateqfive = greateqfive.prefetch(tf.data.AUTOTUNE)\n",
    "    subtr = subtr.batch(batchsize)\n",
    "    subtr = subtr.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # return BOTH datasets\n",
    "    return greateqfive, subtr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n"
     ]
    }
   ],
   "source": [
    "# Check data pipeline by examining one example from each of the four created datasets (one for each math problem for train and test)\n",
    "\n",
    "train_ds_gef, train_ds_subtr = prepare_data(train_ds, batchsize = 32)\n",
    "test_ds_gef, test_ds_subtr = prepare_data(test_ds, batchsize = 32)\n",
    "\n",
    "for img1, img2, label in train_ds_gef.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in train_ds_subtr.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in test_ds_gef.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in test_ds_subtr.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 3: Building Shared Weight Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a suggestion:\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, subtask, optimizer): # numlayers, subtask):\n",
    "\n",
    "        '''\n",
    "        param: numlayers - the desired number of hidden layers\n",
    "        param: subtask - the subtask the network is being asked to solve (relevant for output layer)\n",
    "        '''\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.optimizer = optimizer # tf.keras.optimizers.Adam()\n",
    "        self.subtask = subtask\n",
    "        # self.numlayers = numlayers\n",
    "        # self.layer_list = []\n",
    "\n",
    "        # self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # create 2 hidden layers with 256 units and ReLU as the activation function\n",
    "        self.hidden_layer_1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.hidden_layer_2 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "\n",
    "        # add desired number of hidden layers\n",
    "        # for i in range(numlayers):\n",
    "        #     self.layer_list.append(tf.keras.layers.Dense(units=256, activation=tf.nn.relu))\n",
    "        \n",
    "        if subtask == 0:\n",
    "            self.output_layer = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)\n",
    "            self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "        elif subtask == 1:\n",
    "            self.output_layer = tf.keras.layers.Dense(units=1, activation=tf.nn.softmax) # not 10 units, since the label.shape is (32,) not (32,10)\n",
    "            self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        self.metrics_list = [\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"acc\"), # only for subtask 0, not for subtask 1\n",
    "                    # tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
    "                    ]\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, input: tuple, training = False):\n",
    "        \n",
    "        # # feed both inputs seperatedly into a layer, then concatenate the results before passing activity to the next layer\n",
    "        # i1 = self.flatten(input[0])\n",
    "        i1 = self.hidden_layer_1(input[0])\n",
    "        i1 = self.hidden_layer_2(i1)\n",
    "\n",
    "        # i2 = self.flatten(input[1])\n",
    "        i2 = self.hidden_layer_1(input[1])\n",
    "        i2 = self.hidden_layer_2(i2)\n",
    "\n",
    "        i = tf.concat([i1, i2], axis=1)    \n",
    "\n",
    "        # i1 = self.flatten(input[0])\n",
    "        # i2 = self.flatten(input[1])\n",
    "        \n",
    "        # feed the activity through the network UP TO the output layer\n",
    "        # for numlayers in self.layer_list:\n",
    "        #     if numlayers == 0:\n",
    "        #         i1 = self.hidden_layer_1(i1)\n",
    "        #         i2 = self.hidden_layer_1(i2)\n",
    "        #         i = tf.concat([i1, i2], axis=1)   # e.g. axis=1: (32,784) + (32,784) -> (32, 1568)\n",
    "        #     else:\n",
    "        #         i = self.layer_list[numlayers](i)\n",
    "\n",
    "        out = self.output_layer(i)\n",
    "\n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input):\n",
    "        img1, img2, label = input\n",
    "        print(\"train begins\")\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self((img1, img2), training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "\n",
    "        img1, img2, label = input\n",
    "\n",
    "        prediction = self((img1, img2), training=False)\n",
    "        loss = self.loss_function(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 4: Training the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 2\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training(subtask: int, optimizer): # numlayers: int, ):#, optimizer):\n",
    "    '''\n",
    "    :param subtask: defines the subtask to be solved, 0 is a + b >= 5, 1 is a - b = y\n",
    "    :param optimizer: the optimizer function to use\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # make the if statement\n",
    "\n",
    "    # Initiate a model with the requested parameters\n",
    "    network = MyModel(subtask, optimizer) # numlayers, subtask)\n",
    "\n",
    "    # Initialize the datasets for the two problems\n",
    "\n",
    "    # Note - ignore the fact that train_ds and test_ds may be flagged as not defined; when the whole program is run, this should not be an issue\n",
    "    # train_ds_gef, train_ds_subtr = prepare_data(train_ds, batchsize = 32)\n",
    "    # test_ds_gef, test_ds_subtr = prepare_data(test_ds, batchsize = 32)\n",
    "\n",
    "    # train = train_ds_gef\n",
    "    # test = test_ds_gef\n",
    "\n",
    "    # Initialize the train and test datasets, and the loss function, based on the subtask\n",
    "    if subtask == 0:\n",
    "        train = train_ds_gef\n",
    "        test = test_ds_gef\n",
    "\n",
    "    else:\n",
    "        train = train_ds_subtr\n",
    "        test = test_ds_subtr\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting Epoch {epoch}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(),step=epoch)\n",
    "\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 102.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.17579670250415802', 'acc: 0.9319666624069214']\n",
      "['val_loss: 0.13314342498779297', 'val_acc: 0.9517999887466431']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 155.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.12372072786092758', 'acc: 0.95496666431427']\n",
      "['val_loss: 0.13623151183128357', 'val_acc: 0.9467999935150146']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1875 [00:00<15:53,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 134.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.24911729991436005', 'acc: 0.8996666669845581']\n",
      "['val_loss: 0.19633346796035767', 'val_acc: 0.9247999787330627']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 158.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.18332628905773163', 'acc: 0.9274166822433472']\n",
      "['val_loss: 0.1577572077512741', 'val_acc: 0.9419999718666077']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1875 [00:00<15:46,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 143.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.2229168713092804', 'acc: 0.9102333188056946']\n",
      "['val_loss: 0.1666749268770218', 'val_acc: 0.9337000250816345']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 155.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.15060395002365112', 'acc: 0.94118332862854']\n",
      "['val_loss: 0.13546736538410187', 'val_acc: 0.9470000267028809']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 136.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.18835093080997467', 'acc: 0.9263499975204468']\n",
      "['val_loss: 0.21172402799129486', 'val_acc: 0.9398999810218811']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 140.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.1318303346633911', 'acc: 0.9547333121299744']\n",
      "['val_loss: 0.12684421241283417', 'val_acc: 0.9545000195503235']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1875 [00:00<15:56,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 149.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.300747811794281', 'acc: 0.8759499788284302']\n",
      "['val_loss: 0.24697284400463104', 'val_acc: 0.8970999717712402']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 149.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.24272911250591278', 'acc: 0.9045000076293945']\n",
      "['val_loss: 0.22105538845062256', 'val_acc: 0.9161999821662903']\n"
     ]
    }
   ],
   "source": [
    "# Train a model to solve the first math problem\n",
    "training(0, tf.keras.optimizers.Adam())\n",
    "training(0, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.0))\n",
    "training(0, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.5))\n",
    "training(0, tf.keras.optimizers.RMSprop())\n",
    "training(0, tf.keras.optimizers.Adagrad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:13<00:00, 143.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.715633392333984', 'acc: 0.09103333204984665']\n",
      "['val_loss: 17.74460792541504', 'val_acc: 0.08479999750852585']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 90.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.714799880981445', 'acc: 0.090549997985363']\n",
      "['val_loss: 17.6551513671875', 'val_acc: 0.08730000257492065']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 85.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.745433807373047', 'acc: 0.09003333002328873']\n",
      "['val_loss: 17.745107650756836', 'val_acc: 0.0925000011920929']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 90.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.7009334564209', 'acc: 0.08906666934490204']\n",
      "['val_loss: 17.787939071655273', 'val_acc: 0.09139999747276306']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:20<00:00, 91.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.610933303833008', 'acc: 0.09061666578054428']\n",
      "['val_loss: 17.755290985107422', 'val_acc: 0.09109999984502792']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 100.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.779666900634766', 'acc: 0.09123333543539047']\n",
      "['val_loss: 17.68061065673828', 'val_acc: 0.08900000154972076']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 87.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.64973258972168', 'acc: 0.09128333628177643']\n",
      "['val_loss: 17.549320220947266', 'val_acc: 0.09480000287294388']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:21<00:00, 88.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.678966522216797', 'acc: 0.08951666951179504']\n",
      "['val_loss: 17.660642623901367', 'val_acc: 0.0860000029206276']\n",
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:25<00:00, 72.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.691165924072266', 'acc: 0.08833333104848862']\n",
      "['val_loss: 17.84664535522461', 'val_acc: 0.08950000256299973']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:22<00:00, 83.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 17.691967010498047', 'acc: 0.08953333646059036']\n",
      "['val_loss: 17.899660110473633', 'val_acc: 0.08940000087022781']\n"
     ]
    }
   ],
   "source": [
    "# Train a model to solve the second math problem\n",
    "training(1, tf.keras.optimizers.Adam())\n",
    "training(1, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.0))\n",
    "training(1, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.5))\n",
    "training(1, tf.keras.optimizers.RMSprop())training(1, tf.keras.optimizers.Adagrad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the loss is too high and the acc is too low for the 2nd math prob, need to figure out why & visualization is not done yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 5 - Experiments\n",
    "\n",
    "Run training w/ classic SGD (no momentum)\n",
    "\n",
    "Run training w/ Adam\n",
    "\n",
    "Run training w/ SGD + Momentum\n",
    "\n",
    "Run training w/ RMSrop\n",
    "\n",
    "Run training w/ AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prizl\\Documents\\GitHub\\IANNwTFGr10\\Homework\\04\\Gr10HW04.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Visualize the results of the above training runs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# NEED TO BE WORKED ON\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m5\u001b[39m) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m fig\u001b[39m.\u001b[39msuptitle(\u001b[39m'\u001b[39m\u001b[39mVertically stacked subplots\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, epoch)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the results of the above training runs\n",
    "\n",
    "# NEED TO BE WORKED ON\n",
    "fig, axs = plt.subplots(5) \n",
    "fig.suptitle('Vertically stacked subplots')\n",
    "x = np.linspace(0, epoch)\n",
    "y = np.sin(x ** 2)\n",
    "ax1.plot(x, y)\n",
    "ax2.plot(x, -y)\n",
    "\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(test_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(test_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1, line2, line3, line4),(\"Training Loss\", \"Test Loss\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "fig.savefig(\"Title-Of-The-Figure\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
