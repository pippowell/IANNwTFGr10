{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IANNwTF HW 4\n",
    "## Group 10\n",
    "\n",
    "The following contains our solution to the exercises in IANNwTF HW 04. A Jupyter notebook versus a module format was chosen this time for purposes of organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assigment 1: Reviews\n",
    "We review the homeworks for Groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 2: MNIST Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Preparing the MNIST Math Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Needed Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Load Dataset\n",
    "(train_ds, test_ds), ds_info = tfds.load ('mnist', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "# Info on the dataset (refresher)\n",
    "# print(\"ds_info: \\n\", ds_info)\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.2 Data Pipeline\n",
    "def prepare_data(dataset, batchsize):\n",
    "\n",
    "    '''\n",
    "    :param dataset: the dataset to be prepared for input into the network\n",
    "    :return: 2 datasets, one each for each of the math problems defined (see below), created after the original database was preprocessed with the\n",
    "    steps below\n",
    "    '''\n",
    "\n",
    "    # Step One - General Preprocessing\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors\n",
    "    dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # Step 2 - Pairing Data Tuples & Respective Parameterized Targets\n",
    "\n",
    "    # create a dataset that contains 2000 samples from the overall dataset paired with 2000 other samples\n",
    "    data = tf.data.Dataset.zip((dataset.shuffle(2000), dataset.shuffle(2000)))\n",
    "\n",
    "    # create the dataset for the first math problem (a + b >= 5) - remembering to cast to int versus boolean!\n",
    "    greateqfive = data.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5))\n",
    "    greateqfive = greateqfive.map(lambda x1, x2, t: (x1, x2, tf.cast(t, tf.int32)))\n",
    "\n",
    "    # create the dataset for the second math problem (a - b = y)\n",
    "    subtr = data.map(lambda x1, x2: (x1[0], x2[0], x1[1]-x2[1]))\n",
    "\n",
    "    # Step 3 - Batching & Prefetching\n",
    "\n",
    "    # run batching and prefetching for both datasets\n",
    "    greateqfive = greateqfive.batch(batchsize)\n",
    "    greateqfive = greateqfive.prefetch(tf.data.AUTOTUNE)\n",
    "    subtr = subtr.batch(batchsize)\n",
    "    subtr = subtr.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # return BOTH datasets\n",
    "    return greateqfive, subtr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n"
     ]
    }
   ],
   "source": [
    "# Check data pipeline by examining one example from each of the four created datasets (one for each math problem for train and test)\n",
    "\n",
    "train_ds_gef, train_ds_subtr = prepare_data(train_ds, batchsize = 32)\n",
    "test_ds_gef, test_ds_subtr = prepare_data(test_ds, batchsize = 32)\n",
    "\n",
    "for img1, img2, label in train_ds_gef.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in train_ds_subtr.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in test_ds_gef.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in test_ds_subtr.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 3: Building Shared Weight Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a suggestion:\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self): #, subtask): # numlayers, subtask):\n",
    "\n",
    "        '''\n",
    "        param: numlayers - the desired number of hidden layers\n",
    "        param: subtask - the subtask the network is being asked to solve (relevant for output layer)\n",
    "        '''\n",
    "    \t\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        # create 2 hidden layers with 256 units and ReLU as the activation function\n",
    "        self.hidden_layer_1 = Dense(units=256, activation=tf.nn.relu)\n",
    "        self.hidden_layer_2 = Dense(units=256, activation=tf.nn.relu)\n",
    "        \n",
    "        self.output_layer = Dense(units=1, activation=tf.nn.sigmoid)\n",
    "        # add desired number of hidden layers\n",
    "        # for i in range(numlayers):\n",
    "        #     self.layers.append(Dense(units=256, activation=tf.nn.relu))\n",
    "\n",
    "        # self.subtask = subtask\n",
    "\n",
    "        self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        # if subtask == 0:\n",
    "        #     self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "        # else:\n",
    "        #     self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        self.metrics_list = [\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"acc\"), # only for subtask 0, not for subtask 1\n",
    "                    # tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\") \n",
    "                    ]\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, input: tuple, training = False):\n",
    "        \n",
    "        # feed both inputs seperatedly into a layer, then concatenate the results before passing activity to the next layer\n",
    "\n",
    "        i1 = self.hidden_layer_1(input[0])\n",
    "        i2 = self.hidden_layer_1(input[1])\n",
    "        \n",
    "        i1 = self.hidden_layer_2(i1)\n",
    "        i2 = self.hidden_layer_2(i2)\n",
    "\n",
    "        i = tf.concat([i1, i2], axis=1)    # e.g. axis=1: (32,784) + (32,784) -> (32, 1568)\n",
    "        # i = self.hidden_layer_2(i)\n",
    "\n",
    "        signal = self.output_layer(i)\n",
    "\n",
    "        # feed the activity through the network UP TO the output layer\n",
    "        # for i in self.layers:\n",
    "        #     if i == 0:\n",
    "        #         i1 = self.layers[i](input[0])\n",
    "        #         i2 = self.layers[i](input[1])\n",
    "        #         signal = tf.concat([i1, i2], axis=0)\n",
    "        #     else:\n",
    "        #         signal = self.layers[i](signal)\n",
    "\n",
    "\n",
    "\n",
    "        # create an output layer based on the\n",
    "        # if self.subtask == 0:\n",
    "        #     self.output_layer = Dense(units=2, activation=tf.nn.sigmoid)\n",
    "        #     signal = self.output_layer(i)\n",
    "        # elif self.subtask == 1:\n",
    "        #     self.output_layer = Dense(units=10, activation=tf.nn.softmax)\n",
    "        #     signal = self.output_layer(i)\n",
    "\n",
    "        return signal\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input):\n",
    "        img1, img2, label = input\n",
    "        print(\"train begins\")\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(\"prediction starts\")\n",
    "            prediction = self((img1, img2), training=True) ###????\n",
    "            print(\"first prediction\", prediction)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "\n",
    "        img1, img2, label = input\n",
    "\n",
    "        prediction = self((img1, img2), training=False)\n",
    "        loss = self.loss_function(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 4: Training the Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training():#subtask: int):#, optimizer):\n",
    "    '''\n",
    "    :param subtask: defines the subtask to be solved, 0 is a + b >= 5, 1 is a - b = y\n",
    "    :param optimizer: the optimizer function to use\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # make the if statement\n",
    "\n",
    "    # Initiate a model with the requested parameters\n",
    "    network = MyModel()#subtask)\n",
    "\n",
    "    # Initialize the datasets for the two problems\n",
    "\n",
    "    # Note - ignore the fact that train_ds and test_ds may be flagged as not defined; when the whole program is run, this should not be an issue\n",
    "    # train_ds_gef, train_ds_subtr = prepare_data(train_ds, batchsize = 32)\n",
    "    # test_ds_gef, test_ds_subtr = prepare_data(test_ds, batchsize = 32)\n",
    "\n",
    "    train = train_ds_gef\n",
    "    test = test_ds_gef\n",
    "\n",
    "    # Initialize the train and test datasets, and the loss function, based on the subtask\n",
    "    # if subtask == 0:\n",
    "    #     train = train_ds_gef\n",
    "    #     test = test_ds_gef\n",
    "    #     # network.loss_function = tf.keras.losses.BinaryCrossentropy\n",
    "\n",
    "    # else:\n",
    "    #     train = train_ds_subtr\n",
    "    #     test = test_ds_subtr\n",
    "        # network.loss_function = tf.keras.losses.MeanSquaredError\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Starting Epoch {epoch}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(),step=epoch)\n",
    "\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key,value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n",
      "train begins\n",
      "prediction starts\n",
      "first prediction Tensor(\"StatefulPartitionedCall:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:18<00:00, 102.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.18152807652950287', 'acc: 0.9291166663169861']\n",
      "['val_loss: 0.13546493649482727', 'val_acc: 0.9487000107765198']\n",
      "Starting Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 149.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.12244006246328354', 'acc: 0.9562000036239624']\n",
      "['val_loss: 0.12382649630308151', 'val_acc: 0.9580000042915344']\n",
      "Starting Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 159.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.10534211248159409', 'acc: 0.9646499752998352']\n",
      "['val_loss: 0.10426236689090729', 'val_acc: 0.9641000032424927']\n",
      "Starting Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 154.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.09758567810058594', 'acc: 0.9687666893005371']\n",
      "['val_loss: 0.12065297365188599', 'val_acc: 0.9571999907493591']\n",
      "Starting Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 156.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.09390214830636978', 'acc: 0.9699333310127258']\n",
      "['val_loss: 0.09814545512199402', 'val_acc: 0.9718000292778015']\n",
      "Starting Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 159.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.0889250785112381', 'acc: 0.9728500247001648']\n",
      "['val_loss: 0.09208716452121735', 'val_acc: 0.9729999899864197']\n",
      "Starting Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 159.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.08458562940359116', 'acc: 0.9741666913032532']\n",
      "['val_loss: 0.087535560131073', 'val_acc: 0.9735999703407288']\n",
      "Starting Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:12<00:00, 155.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.08216635137796402', 'acc: 0.9748166799545288']\n",
      "['val_loss: 0.09740924835205078', 'val_acc: 0.972100019454956']\n",
      "Starting Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 159.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.07898574322462082', 'acc: 0.9768999814987183']\n",
      "['val_loss: 0.09125948697328568', 'val_acc: 0.9696999788284302']\n",
      "Starting Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:11<00:00, 158.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.07719042152166367', 'acc: 0.9774166941642761']\n",
      "['val_loss: 0.08818335086107254', 'val_acc: 0.9779999852180481']\n"
     ]
    }
   ],
   "source": [
    "# Train a model to solve the first math problem\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "training() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prizl\\Documents\\GitHub\\IANNwTFGr10\\Homework\\04\\Gr10HW04.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train a model to solve the second math problem\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/04/Gr10HW04.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m training(\u001b[39m1\u001b[39;49m, tf\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam)\n",
      "\u001b[1;31mTypeError\u001b[0m: training() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "# Train a model to solve the second math problem\n",
    "training(1, tf.optimizers.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 5 - Experiments\n",
    "\n",
    "Run training w/ classic SGD (no momentum)\n",
    "\n",
    "Run training w/ Adam\n",
    "\n",
    "Run training w/ SGD + Momentum\n",
    "\n",
    "Run training w/ RMSrop\n",
    "\n",
    "Run training w/ AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the results of the above training runs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
