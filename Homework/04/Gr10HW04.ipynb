{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# IANNwTF HW 4\n",
    "## Group 10\n",
    "\n",
    "The following contains our solution to the exercises in IANNwTF HW 04. A Jupyter notebook versus a module format was chosen this time for purposes of organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assigment 1: Reviews\n",
    "We review the homeworks for Groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 2: MNIST Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Preparing the MNIST Math Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Needed Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tqdm\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.1 Load Dataset\n",
    "(train_ds, test_ds), ds_info = tfds.load ('mnist', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "# Info on the dataset (refresher)\n",
    "# print(\"ds_info: \\n\", ds_info)\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.2 Data Pipeline\n",
    "def prepare_data(dataset, batchsize):\n",
    "\n",
    "    '''\n",
    "    :param dataset: the dataset to be prepared for input into the network\n",
    "    :param batchsize: the desired batchsize\n",
    "    :return: 2 datasets, one each for each of the math problems defined (see below), created after the original database was preprocessed with the\n",
    "    steps below\n",
    "    '''\n",
    "\n",
    "    # Step One - General Preprocessing\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors\n",
    "    dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # Step 2 - Pairing Data Tuples & Respective Parameterized Targets\n",
    "\n",
    "    # create a dataset that contains 2000 samples from the overall dataset paired with 2000 other samples\n",
    "    data = tf.data.Dataset.zip((dataset.shuffle(2000), dataset.shuffle(2000)))\n",
    "\n",
    "    # create the dataset for the first math problem (a + b >= 5) - remembering to cast to int versus boolean!\n",
    "    greateqfive = data.map(lambda x1, x2: (x1[0], x2[0], x1[1]+x2[1]>=5))\n",
    "    greateqfive = greateqfive.map(lambda x1, x2, t: (x1, x2, tf.cast(t, tf.int32)))\n",
    "\n",
    "    # create the dataset for the second math problem (a - b = y)\n",
    "    subtr = data.map(lambda x1, x2: (x1[0], x2[0], x1[1]-x2[1]))\n",
    "\n",
    "    # Step 3 - Batching & Prefetching\n",
    "\n",
    "    # run batching and prefetching for both datasets\n",
    "    greateqfive = greateqfive.batch(batchsize)\n",
    "    greateqfive = greateqfive.prefetch(tf.data.AUTOTUNE)\n",
    "    subtr = subtr.batch(batchsize)\n",
    "    subtr = subtr.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # return BOTH datasets\n",
    "    return greateqfive, subtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n",
      "(32, 784) (32, 784) (32,)\n"
     ]
    }
   ],
   "source": [
    "# Check data pipeline by examining one example from each of the four created datasets (one for each math problem for train and test)\n",
    "\n",
    "train_ds_gef, train_ds_subtr = prepare_data(train_ds, batchsize = 32)\n",
    "test_ds_gef, test_ds_subtr = prepare_data(test_ds, batchsize = 32)\n",
    "\n",
    "for img1, img2, label in train_ds_gef.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in train_ds_subtr.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in test_ds_gef.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "\n",
    "for img1, img2, label in test_ds_subtr.take(1):\n",
    "    print(img1.shape, img2.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 5\n",
    "learning_rate = 0.1 # 0.01\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 3: Building Shared Weight Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, numlayers, subtask, optimizer):\n",
    "\n",
    "        '''\n",
    "        param: numlayers - the desired number of hidden layers\n",
    "        param: subtask - the subtask the network is being asked to solve (relevant for output layer)\n",
    "        param: optimizer - the optimizer to be used\n",
    "        '''\n",
    "\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.subtask = subtask\n",
    "\n",
    "        self.layerlist = [Dense(layers,activation=\"relu\") for layers in range(numlayers)]\n",
    "\n",
    "        if subtask == 0:\n",
    "            self.output_layer = Dense(units=1, activation=tf.nn.sigmoid)\n",
    "            self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
    "        elif subtask == 1:\n",
    "            self.output_layer = Dense(units=1, activation=tf.nn.softmax) # not 10 units, since the label.shape is (32,) not (32,10)\n",
    "            self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        self.metrics_list = [\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"acc\"), # only for subtask 0, not for subtask 1\n",
    "                    #tf.keras.metrics.TopKCategoricalAccuracy(3,name=\"top-3-acc\")\n",
    "                    ]\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, input: tuple, training = False):\n",
    "\n",
    "        # feed both inputs separately into the first layer, then concatenate the results before passing activity through the rest of the network UP TO the output layer\n",
    "        for layer in range(len(self.layerlist)):\n",
    "             if layer == 0:\n",
    "                 i1 = self.layerlist[layer](input[0])\n",
    "                 i2 = self.layerlist[layer](input[1])\n",
    "                 i = tf.concat([i1, i2], axis=1)   # e.g. axis=1: (32,784) + (32,784) -> (32, 1568)\n",
    "             else:\n",
    "                 i = self.layerlist[layer](i)\n",
    "\n",
    "        # run the activity through the output layer after it passes through the hidden layers\n",
    "        output = self.output_layer(i)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input):\n",
    "        img1, img2, label = input\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self((img1, img2), training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "\n",
    "        img1, img2, label = input\n",
    "\n",
    "        prediction = self((img1, img2), training=False)\n",
    "        loss = self.loss_function(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Assignment 4: Training the Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training(numlayers, subtask, optimizer):\n",
    "    '''\n",
    "    :param: numlayers: the number of layers desired in the network to be trained\n",
    "    :param subtask: defines the subtask to be solved, 0 is a + b >= 5, 1 is a - b = y\n",
    "    :param optimizer: the optimizer function to use\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Initiate a model with the requested parameters\n",
    "    network = MyModel(numlayers, subtask, optimizer)\n",
    "\n",
    "    # Initialize the train and test datasets, and the loss function, based on the subtask\n",
    "    if subtask == 0:\n",
    "        train = train_ds_gef\n",
    "        test = test_ds_gef\n",
    "\n",
    "    else:\n",
    "        train = train_ds_subtr\n",
    "        test = test_ds_subtr\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # Piper Attempt at Retrieve Mets for Vis\n",
    "        #train_losses.append(network.metrics[0].result())\n",
    "        #train_accuracies.append(network.metrics[1].result())\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # wooki's struggle to make a list for the visualization (pls don't delete)\n",
    "        #print(metrics)\n",
    "        #train_losses.append(metrics.values.numpy()[1])\n",
    "        #train_accuracies.append(metrics.values.numpy()[1])\n",
    "        # for (key, value) in metrics:\n",
    "\n",
    "            # if key == \"loss\":\n",
    "            #     train_losses.append(value.numpy())\n",
    "            # elif key == \"acc\":\n",
    "            #     train_accuracies.append(value.numpy())\n",
    "            \n",
    "        # pls don't delete until here\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # Piper Attempt at Retrieve Mets for Vis\n",
    "        #test_losses.append(network.metrics[2].result())\n",
    "        #test_accuracies.append(network.metrics[3].result())\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()\n",
    "\n",
    "        #return train_losses,train_accuracies,test_losses,test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 207.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 0.5251139402389526', 'train_acc: 0.8405666947364807']\n",
      "['val_loss: 0.457953542470932', 'val_acc: 0.835099995136261']\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:07<00:00, 243.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 0.44517287611961365', 'train_acc: 0.8381500244140625']\n",
      "['val_loss: 0.4467918574810028', 'val_acc: 0.8356999754905701']\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:07<00:00, 237.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 0.4387131631374359', 'train_acc: 0.8406000137329102']\n",
      "['val_loss: 0.44238734245300293', 'val_acc: 0.8381999731063843']\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:08<00:00, 225.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 0.44103941321372986', 'train_acc: 0.8391833305358887']\n",
      "['val_loss: 0.44690239429473877', 'val_acc: 0.8356000185012817']\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:07<00:00, 236.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 0.43913599848747253', 'train_acc: 0.8403333425521851']\n",
      "['val_loss: 0.43809232115745544', 'val_acc: 0.8410000205039978']\n"
     ]
    }
   ],
   "source": [
    "# Train a model to solve the first math problem\n",
    "training(2, 0, tf.keras.optimizers.Adam())\n",
    "#training(2, 0, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.0))\n",
    "#training(2, 0, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.5))\n",
    "#training(2, 0, tf.keras.optimizers.RMSprop())\n",
    "#training(2, 0, tf.keras.optimizers.Adagrad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:09<00:00, 192.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 17.72833251953125', 'train_acc: 0.08996666967868805']\n",
      "['val_loss: 17.72484016418457', 'val_acc: 0.08820000290870667']\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:07<00:00, 235.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 17.66116714477539', 'train_acc: 0.08966666460037231']\n",
      "['val_loss: 17.768171310424805', 'val_acc: 0.09130000323057175']\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:08<00:00, 217.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 17.676265716552734', 'train_acc: 0.08996666967868805']\n",
      "['val_loss: 17.715755462646484', 'val_acc: 0.09179999679327011']\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:07<00:00, 236.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 17.716867446899414', 'train_acc: 0.09125000238418579']\n",
      "['val_loss: 17.947484970092773', 'val_acc: 0.08609999716281891']\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:08<00:00, 228.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 17.70676612854004', 'train_acc: 0.09099999815225601']\n",
      "['val_loss: 17.95946502685547', 'val_acc: 0.08799999952316284']\n"
     ]
    }
   ],
   "source": [
    "# Train a model to solve the second math problem\n",
    "training(2, 1, tf.keras.optimizers.Adam())\n",
    "#training(2, 1, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.0))\n",
    "#training(2, 1, tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.5))\n",
    "#training(2, 1, tf.keras.optimizers.RMSprop())\n",
    "#training(2, 1, tf.keras.optimizers.Adagrad())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy does initially seem to go up over the epochs, but then drops again. But then this is the harder task I think. Visualization should be a matter of pulling out the metric results into the lists we created so we can use those lists for the next part. I tried this with the code I added above (marked as my attempt), but no luck.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Assignment 5 - Experiments\n",
    "\n",
    "Run training w/ classic SGD (no momentum)\n",
    "\n",
    "Run training w/ Adam\n",
    "\n",
    "Run training w/ SGD + Momentum\n",
    "\n",
    "Run training w/ RMSrop\n",
    "\n",
    "Run training w/ AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the results of the above training runs\n",
    "\n",
    "# NEED TO BE WORKED ON\n",
    "fig, axs = plt.subplots(5) \n",
    "fig.suptitle('Vertically stacked subplots')\n",
    "x = np.linspace(0, epochs)\n",
    "y = np.sin(x ** 2)\n",
    "ax1.plot(x, y)\n",
    "ax2.plot(x, -y)\n",
    "\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(test_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(test_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1, line2, line3, line4),(\"Training Loss\", \"Test Loss\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "fig.savefig(\"Title-Of-The-Figure\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}