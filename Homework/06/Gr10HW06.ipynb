{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Homework 6\n",
    " ## Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 1 - Reviews\n",
    "\n",
    "We review the homeworks for groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 2 - CIFAR-10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## READ ME\n",
    "\n",
    "This notebook does not work like a standard notebook. It is instead dynamic, allowing you to decide what optimization techniques (or combinations of them) you wish to apply. Follow the instructions in bold throughout this sheet to ensure you do not miss any required cells and that you fully understand your options (the menu) for each customizable section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run all of the following cells until you encounter a menu option instruction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prizl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# the necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tqdm\n",
    "import keras_cv # install keras_cv with pip for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "        # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_aug(dataset):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])\n",
    "\n",
    "    dataset = dataset.map(lambda x, y : (augmentation_model(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load ('cifar10', split =['train', 'test'], as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Appetizers - Dataset Options\n",
    "\n",
    "**You have two options for this part of the menu:**\n",
    "\n",
    "**- Option 1: standard preprocessing for the CIFAR-10 dataset**\n",
    "**- Option 2: preprocessing w/ data augmentation for this dataset**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "**If you would like to see a readout and/or examples from the raw dataset, uncomment the lines in the cell immediately following this message.**\n",
    "\n",
    "*Note - this section and the two preprocessing functions defined above fulfill part 2.1 of the assignment.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_info: \n",
      " tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='C:\\\\Users\\\\powel\\\\tensorflow_datasets\\\\cifar10\\\\3.0.2',\n",
      "    file_format=tfrecord,\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"ds_info: \\n\", ds_info)\n",
    "# visualize a sample of the dataset\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - Just the Standard Please\n",
    "\n",
    "train_dataset = preprocess(train_ds)\n",
    "test_dataset = preprocess(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - Augment My Meal\n",
    "\n",
    "train_dataset_aug = preprocess_aug(train_ds)\n",
    "test_dataset_aug = preprocess_aug(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the following cell regardless. It defines the basic CNN model we will be using. Don't worry, you'll get to play around with optimization options for it later. :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    def __init__(self, L1_reg=0, L2_reg=0, dropout_rate=0, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        if L2_reg >= 0: # L2 is the default\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L2_reg)\n",
    "        elif L1_reg > 0: \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L1_reg)\n",
    "        else:\n",
    "            None\n",
    "\n",
    "        if self.dropout_rate:\n",
    "            self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "        # input 32x32x3 with 3 as the color channels\n",
    "        if batch_norm:\n",
    "            self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.GlobalAvgPool2D(),\n",
    "                                tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "\n",
    "        else:\n",
    "            self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.GlobalAvgPool2D(),\n",
    "                                tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "\n",
    "        self.metrics_list = [ \n",
    "                    tf.keras.metrics.Mean(name=\"total_frobenius_norm\"),\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                    ]\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.dropout_rate:\n",
    "                x = self.dropout_layer(x, training)\n",
    "\n",
    "        return self.layer_list[-1](x)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "\n",
    "        img, label = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(img, training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "            # loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.metrics[0].update_state(self.compute_frobenius())\n",
    "        self.metrics[1].update_state(loss)\n",
    "        self.metrics[2].update_state(label, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "\n",
    "        img, label = data\n",
    "\n",
    "        prediction = self(img, training=False)\n",
    "        loss = self.loss_function(label, prediction)\n",
    "        # loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "\n",
    "        self.metrics[0].update_state(self.compute_frobenius())\n",
    "        self.metrics[1].update_state(loss)\n",
    "        self.metrics[2].update_state(label, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First Course - In-Model Optimization Options\n",
    "\n",
    "**You have four options for this part of the menu:**\n",
    "\n",
    "**- Option 1: the basic model w/ no optimizations applied**\n",
    "**- Option 2: the basic model w/ L1 regularization**\n",
    "**- Option 3: the basic model w/ L2 regularization**\n",
    "**- Option 4: the basic model w/ dropout layers**\n",
    "**- Option 5: the basic model w/ batch normalization**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "*Note - this section and the model code above fulfill part 2.2 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 0 - Plane Jane\n",
    "\n",
    "network_original = ConvModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1 - Lovin' L1\n",
    "\n",
    "network_L1 = ConvModel(L1_reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - Lovin' L2\n",
    "\n",
    "network_L2 = ConvModel(L2_reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 3 - Drop It\n",
    "\n",
    "network_dropout = ConvModel(dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 4 - Nabbin' the Norm\n",
    "\n",
    "network_batchnorm = ConvModel(batch_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 5 - Data Augmentation\n",
    "augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])\n",
    "network_aug = ConvModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the next cell regardless. It sets up the logging and metrics we'll need for visualization later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"HW06\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 2 #15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_forb_norm = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "val_forb_norm = [] # Q. do we need both?\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Second Course - Training Options\n",
    "\n",
    "**You have two options for this part of the menu:**\n",
    "\n",
    "**- Option 1: standard training**\n",
    "**- Option 2: training that implements early stopping**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "*Note - this section fulfills part 2.3 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1 - The Usual\n",
    "\n",
    "def training_1(network):\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                train_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                val_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_1(network_original)\n",
    "training_1(network_L1)\n",
    "training_1(network_L2)\n",
    "training_1(network_dropout)\n",
    "training_1(network_batchnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 - for data aug\n",
    "\n",
    "def training_2(network):\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset_aug, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                train_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset_aug:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                val_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_2(network_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 3 - The Early Bird\n",
    "\n",
    "def training_es(network):\n",
    "\n",
    "    # define a counter for the number of epochs when the validation loss increased\n",
    "    lossincreasecount = 0\n",
    "\n",
    "    # define how many epochs you will run for at the start before caring whether validation loss is increasing\n",
    "    guaranteedepochs = 1\n",
    "\n",
    "    # define how many consecutive epochs you will tolerate the validation loss increasing for before halting training\n",
    "    tolerableincreaseepochs = 1\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # always run training for the guaranteed number of epochs defined above\n",
    "        if epoch < guaranteedepochs:\n",
    "            print('gauranteed epoch')\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "        # after the guaranteed number of epochs, start monitoring val loss to determine when training should stop\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # if you see the validation loss increasing for more than a set amount of consecutive epochs, terminate training\n",
    "            if val_losses[epoch]>val_losses[epoch-1] and lossincreasecount != tolerableincreaseepochs:\n",
    "                lossincreasecount=++1\n",
    "                print('val loss has increased for' + lossincreasecount + ' epochs.')\n",
    "\n",
    "            if val_losses[epoch]>val_losses[epoch-1] and lossincreasecount == tolerableincreaseepochs:\n",
    "                print('val loss still increasing beyond tolerated epoch number, halting training')\n",
    "                break\n",
    "\n",
    "            elif val_losses[epoch]<=val_losses[epoch-1]:\n",
    "                print('val loss decreased or stayed the same, giving it another chance')\n",
    "                lossincreasecount = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the following two cells regardless in order to train the network with the selected menu options.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare the tensorboard ahead of training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Restaurant Review - Visualization\n",
    "\n",
    "**Run the cell below to see how your model performed with the selected menu options.**\n",
    "\n",
    "*Note - this section fulfills part 2.4 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "fig = plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(val_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2,line3,line4),(\"Training Loss\",\"Test Loss\",\"Training Accuracy\",\"Test Accuracy\"))\n",
    "fig.savefig(\"CNN Performance CIFAR-10\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overfitting Report\n",
    "\n",
    "Our original model overfit the data, as evidenced by the plateau in validation (testing) performance versus training. This discrepancy indicates it began to use features in the training data which did not help it generalize when it saw the new data in the test batch.\n",
    "\n",
    "## Optimization Report\n",
    "\n",
    "We attempted the following optimization techniques and report on our reasoning for them and the results we obtained with them below:\n",
    "\n",
    "Data Augmentation:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "L1 Regularization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "L2 Regularization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Dropout:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Batch Normalization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Early Stopping :\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Old Plotting Code when Fit and Compile Were Used - Piper didn't want to delete this without permission. :)\n",
    "\n",
    "# plotting\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(6, 1, figsize=(8, 10))\n",
    "\n",
    "ax0.set_title(\"original\")\n",
    "ax0.plot(original.history[\"total_frobenius_norm\"]/np.max(original.history[\"total_frobenius_norm\"]) * np.max(original.history[\"val_loss\"]))\n",
    "ax0.plot(original.history[\"val_loss\"])\n",
    "ax0.plot(original.history[\"loss\"])\n",
    "ax0.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax0.savefig(\"convnet_original\")\n",
    "# ax0.show()\n",
    "\n",
    "ax1.set_title(\"L1\")\n",
    "ax1.plot(lassoReg.history[\"total_frobenius_norm\"]/np.max(lassoReg.history[\"total_frobenius_norm\"]) * np.max(lassoReg.history[\"val_loss\"]))\n",
    "ax1.plot(lassoReg.history[\"val_loss\"])\n",
    "ax1.plot(lassoReg.history[\"loss\"])\n",
    "# ax1.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax1.savefig(\"convnet_L1\")\n",
    "# ax1.show()\n",
    "\n",
    "ax2.set_title(\"L2\")\n",
    "ax2.plot(ridgeReg.history[\"total_frobenius_norm\"]/np.max(ridgeReg.history[\"total_frobenius_norm\"]) * np.max(ridgeReg.history[\"val_loss\"]))\n",
    "ax2.plot(ridgeReg.history[\"val_loss\"])\n",
    "ax2.plot(ridgeReg.history[\"loss\"])\n",
    "# ax2.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax2.savefig(\"convnet_L2\")\n",
    "# ax2.show()\n",
    "\n",
    "ax3.set_title(\"L2\")\n",
    "ax3.plot(augment.history[\"total_frobenius_norm\"]/np.max(augment.history[\"total_frobenius_norm\"]) * np.max(augment.history[\"val_loss\"]))\n",
    "ax3.plot(augment.history[\"val_loss\"])\n",
    "ax3.plot(augment.history[\"loss\"])\n",
    "# ax3.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax3.savefig(\"convnet_augment\")\n",
    "# ax3.show()\n",
    "\n",
    "ax4.set_title(\"Dropout\")\n",
    "ax4.plot(dropout.history[\"total_frobenius_norm\"]/np.max(dropout.history[\"total_frobenius_norm\"]) * np.max(dropout.history[\"val_loss\"]))\n",
    "ax4.plot(dropout.history[\"val_loss\"])\n",
    "ax4.plot(dropout.history[\"loss\"])\n",
    "# ax4.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax4.savefig(\"convnet_dropout\")\n",
    "# ax4.show()\n",
    "\n",
    "ax5.set_title(\"Batch Norm\")\n",
    "ax5.plot(batchnorm.history[\"total_frobenius_norm\"]/np.max(batchnorm.history[\"total_frobenius_norm\"]) * np.max(batchnorm.history[\"val_loss\"]))\n",
    "ax5.plot(batchnorm.history[\"val_loss\"])\n",
    "ax5.plot(batchnorm.history[\"loss\"])\n",
    "# ax5.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax5.savefig(\"convnet_batchnorm\")\n",
    "# ax5.show()\n",
    "\n",
    "ax5.set_xlabel(\"Epochs\")\n",
    "# ax5.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "fig.savefig(\"5 optimization methods comparison (e=15)\")\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
