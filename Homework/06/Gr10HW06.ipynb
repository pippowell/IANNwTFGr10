{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Homework 6\n",
    " ## Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 1 - Reviews\n",
    "\n",
    "We review the homeworks for groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 2 - CIFAR-10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## READ ME\n",
    "\n",
    "This notebook does not work like a standard notebook. It is instead dynamic, allowing you to decide what optimization techniques (or combinations of them) you wish to apply. Follow the instructions in bold throughout this sheet to ensure you do not miss any required cells and that you fully understand your options (the menu) for each customizable section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run all of the following cells until you encounter a menu option instruction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tqdm\n",
    "import keras_cv # install keras_cv with pip for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "        # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_aug(dataset):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])\n",
    "\n",
    "    dataset = dataset.map(lambda x, y : (augmentation_model(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load ('cifar10', split =['train', 'test'], as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Appetizers - Dataset Options\n",
    "\n",
    "**You have two options for this part of the menu:**\n",
    "\n",
    "**- Option 1: standard preprocessing for the CIFAR-10 dataset**\n",
    "**- Option 2: preprocessing w/ data augmentation for this dataset**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "**If you would like to see a readout and/or examples from the raw dataset, uncomment the lines in the cell immediately following this message.**\n",
    "\n",
    "*Note - this section and the two preprocessing functions defined above fulfill part 2.1 of the assignment.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"ds_info: \\n\", ds_info)\n",
    "# visualize a sample of the dataset\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - Just the Standard Please\n",
    "\n",
    "train_dataset = preprocess(train_ds)\n",
    "test_dataset = preprocess(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - Augment My Meal\n",
    "\n",
    "train_dataset = preprocess_aug(train_ds)\n",
    "test_dataset = preprocess_aug(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the following cell regardless. It defines the basic CNN model we will be using. Don't worry, you'll get to play around with optimization options for it later. :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    def __init__(self, L1_reg=0, L2_reg=0, dropout_rate=0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if L2_reg >= 0: # L2 is the default\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L2_reg)\n",
    "        elif L1_reg > 0: \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L1_reg)\n",
    "        else:\n",
    "            None\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if self.dropout_rate:\n",
    "            self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "            \n",
    "        self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "            tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        \n",
    "        if batch_norm:    \n",
    "                self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                   tf.keras.layers.BatchNormalization(), # why was it commented out??\n",
    "                                    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        # metrics to update\n",
    "        # self.frobenius_metric = tf.keras.metrics.Mean(name=\"total_frobenius_norm\")\n",
    "        # self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        # self.accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        self.metrics_list = [ \n",
    "                    tf.keras.metrics.Mean(name=\"total_frobenius_norm\"),\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                    ]\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.dropout_rate:\n",
    "                x = self.dropout_layer(x, training)\n",
    "        \n",
    "        return self.layer_list[-1](x)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(x, training=True)\n",
    "            loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.metrics[0].update_state(self.compute_frobenius())\n",
    "        self.metrics[1].update_state(loss)\n",
    "        self.metrics[0].update_state(x, target)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, target = data\n",
    "        prediction = self(x, training=False)\n",
    "        loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        \n",
    "        # self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        # self.loss_metric.update_state(loss)\n",
    "        # self.accuracy_metric.update_state(target, prediction)\n",
    "\n",
    "        self.metrics[0].update_state(self.compute_frobenius())\n",
    "        self.metrics[1].update_state(loss)\n",
    "        self.metrics[0].update_state(x, target)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First Course - In-Model Optimization Options\n",
    "\n",
    "**You have four options for this part of the menu:**\n",
    "\n",
    "**- Option 1: the basic model w/ no optimizations applied**\n",
    "**- Option 2: the basic model w/ L1 regularization**\n",
    "**- Option 3: the basic model w/ L2 regularization**\n",
    "**- Option 4: the basic model w/ dropout layers**\n",
    "**- Option 5: the basic model w/ batch normalization**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "*Note - this section and the model code above fulfill part 2.2 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1 - Plane Jane\n",
    "\n",
    "network = ConvModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - Lovin' L1\n",
    "\n",
    "network = ConvModel(L1_reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 3 - Lovin' L2\n",
    "\n",
    "network = ConvModel(L2_reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 4 - Drop It\n",
    "\n",
    "network = ConvModel(dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 5 - Nabbin' the Norm\n",
    "\n",
    "network = ConvModel(batch_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the next cell regardless. It sets up the logging and metrics we'll need for visualization later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"HW06\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 2 #15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_forb_norm = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "val_forb_norm = [] # Q. do we need both?\n",
    "val_losses = []\n",
    "val_accuracies = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Second Course - Training Options\n",
    "\n",
    "**You have two options for this part of the menu:**\n",
    "\n",
    "**- Option 1: standard training**\n",
    "**- Option 2: training that implements early stopping**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "*Note - this section fulfills part 2.3 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tqdm.tqdm(train_dataset)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1 - The Usual\n",
    "\n",
    "def training(network):\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            print(data)\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                train_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                val_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - The Early Bird\n",
    "\n",
    "def training_es(network):\n",
    "\n",
    "    # define a counter for the number of epochs when the validation loss increased\n",
    "    lossincreasecount = 0\n",
    "\n",
    "    # define how many epochs you will run for at the start before caring whether validation loss is increasing\n",
    "    guaranteedepochs = 1\n",
    "\n",
    "    # define how many consecutive epochs you will tolerate the validation loss increasing for before halting training\n",
    "    tolerableincreaseepochs = 1\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # always run training for the guaranteed number of epochs defined above\n",
    "        if epoch < guaranteedepochs:\n",
    "            print('gauranteed epoch')\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "        # after the guaranteed number of epochs, start monitoring val loss to determine when training should stop\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # if you see the validation loss increasing for more than a set amount of consecutive epochs, terminate training\n",
    "            if val_losses[epoch]>val_losses[epoch-1] and lossincreasecount != tolerableincreaseepochs:\n",
    "                lossincreasecount=++1\n",
    "                print('val loss has increased for' + lossincreasecount + ' epochs.')\n",
    "\n",
    "            if val_losses[epoch]>val_losses[epoch-1] and lossincreasecount == tolerableincreaseepochs:\n",
    "                print('val loss still increasing beyond tolerated epoch number, halting training')\n",
    "                break\n",
    "\n",
    "            elif val_losses[epoch]<=val_losses[epoch-1]:\n",
    "                print('val loss decreased or stayed the same, giving it another chance')\n",
    "                lossincreasecount = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the following two cells regardless in order to train the network with the selected menu options.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the tensorboard ahead of training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, optimizer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 32, 32, 3), dtype=float32, numpy=\n",
      "array([[[[ 0.09375  , -0.234375 , -0.8984375],\n",
      "         [ 0.1875   , -0.1484375, -0.859375 ],\n",
      "         [ 0.203125 , -0.140625 , -0.875    ],\n",
      "         ...,\n",
      "         [-0.09375  , -0.3671875, -0.9921875],\n",
      "         [-0.0625   , -0.3359375, -0.984375 ],\n",
      "         [-0.0078125, -0.3046875, -0.9765625]],\n",
      "\n",
      "        [[ 0.0625   , -0.2734375, -0.9140625],\n",
      "         [ 0.109375 , -0.2578125, -0.90625  ],\n",
      "         [ 0.09375  , -0.2578125, -0.8984375],\n",
      "         ...,\n",
      "         [-0.125    , -0.3984375, -0.9921875],\n",
      "         [-0.1171875, -0.3828125, -0.9921875],\n",
      "         [-0.1015625, -0.375    , -0.9921875]],\n",
      "\n",
      "        [[ 0.1328125, -0.21875  , -0.8984375],\n",
      "         [ 0.265625 , -0.125    , -0.8515625],\n",
      "         [ 0.25     , -0.1328125, -0.8671875],\n",
      "         ...,\n",
      "         [-0.15625  , -0.4375   , -0.9921875],\n",
      "         [-0.1484375, -0.4375   , -0.9921875],\n",
      "         [-0.1328125, -0.421875 , -0.9765625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0625   , -0.234375 , -0.8046875],\n",
      "         [-0.0703125, -0.234375 , -0.7890625],\n",
      "         [-0.1328125, -0.3046875, -0.8359375],\n",
      "         ...,\n",
      "         [ 0.0390625, -0.1484375, -0.8203125],\n",
      "         [-0.0234375, -0.1953125, -0.8359375],\n",
      "         [-0.0859375, -0.2265625, -0.8515625]],\n",
      "\n",
      "        [[-0.046875 , -0.203125 , -0.8046875],\n",
      "         [-0.015625 , -0.1875   , -0.796875 ],\n",
      "         [ 0.       , -0.1875   , -0.7890625],\n",
      "         ...,\n",
      "         [ 0.0546875, -0.140625 , -0.796875 ],\n",
      "         [-0.015625 , -0.1796875, -0.8125   ],\n",
      "         [-0.1015625, -0.25     , -0.859375 ]],\n",
      "\n",
      "        [[-0.03125  , -0.203125 , -0.828125 ],\n",
      "         [ 0.015625 , -0.15625  , -0.8125   ],\n",
      "         [ 0.0234375, -0.15625  , -0.8203125],\n",
      "         ...,\n",
      "         [-0.015625 , -0.1875   , -0.7734375],\n",
      "         [-0.0546875, -0.2109375, -0.8125   ],\n",
      "         [-0.125    , -0.265625 , -0.8515625]]],\n",
      "\n",
      "\n",
      "       [[[ 0.2890625,  0.1796875,  0.375    ],\n",
      "         [ 0.171875 ,  0.1328125,  0.4609375],\n",
      "         [ 0.125    ,  0.140625 ,  0.578125 ],\n",
      "         ...,\n",
      "         [-0.6640625, -0.859375 , -0.5703125],\n",
      "         [-0.6171875, -0.7890625, -0.546875 ],\n",
      "         [-0.421875 , -0.6015625, -0.390625 ]],\n",
      "\n",
      "        [[ 0.5546875,  0.421875 ,  0.3984375],\n",
      "         [ 0.421875 ,  0.2890625,  0.4375   ],\n",
      "         [ 0.3125   ,  0.15625  ,  0.375    ],\n",
      "         ...,\n",
      "         [-0.640625 , -0.84375  , -0.5625   ],\n",
      "         [-0.625    , -0.8359375, -0.5625   ],\n",
      "         [-0.578125 , -0.7734375, -0.5390625]],\n",
      "\n",
      "        [[ 0.375    ,  0.28125  ,  0.4375   ],\n",
      "         [ 0.359375 ,  0.25     ,  0.4296875],\n",
      "         [ 0.3125   ,  0.1796875,  0.3984375],\n",
      "         ...,\n",
      "         [-0.6328125, -0.8203125, -0.546875 ],\n",
      "         [-0.59375  , -0.78125  , -0.546875 ],\n",
      "         [-0.4921875, -0.6640625, -0.5078125]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.21875  , -0.2265625,  0.1953125],\n",
      "         [-0.171875 , -0.171875 ,  0.2578125],\n",
      "         [-0.15625  , -0.140625 ,  0.3046875],\n",
      "         ...,\n",
      "         [-0.25     , -0.3125   ,  0.078125 ],\n",
      "         [-0.265625 , -0.3359375,  0.03125  ],\n",
      "         [-0.265625 , -0.3671875, -0.03125  ]],\n",
      "\n",
      "        [[-0.3046875, -0.3046875,  0.1328125],\n",
      "         [-0.2734375, -0.2578125,  0.203125 ],\n",
      "         [-0.2109375, -0.1953125,  0.28125  ],\n",
      "         ...,\n",
      "         [-0.2421875, -0.328125 ,  0.0546875],\n",
      "         [-0.25     , -0.3359375,  0.0234375],\n",
      "         [-0.2734375, -0.3671875, -0.046875 ]],\n",
      "\n",
      "        [[-0.328125 , -0.3359375,  0.1015625],\n",
      "         [-0.3125   , -0.296875 ,  0.140625 ],\n",
      "         [-0.25     , -0.234375 ,  0.2265625],\n",
      "         ...,\n",
      "         [-0.265625 , -0.3671875, -0.0078125],\n",
      "         [-0.265625 , -0.3828125, -0.015625 ],\n",
      "         [-0.2890625, -0.40625  , -0.0703125]]],\n",
      "\n",
      "\n",
      "       [[[ 0.9921875,  0.9921875,  0.9921875],\n",
      "         [ 0.9921875,  0.9921875,  0.9921875],\n",
      "         [ 0.9296875,  0.9375   ,  0.9296875],\n",
      "         ...,\n",
      "         [ 0.9609375,  0.9609375,  0.96875  ],\n",
      "         [ 0.9140625,  0.9296875,  0.9296875],\n",
      "         [ 0.90625  ,  0.9140625,  0.921875 ]],\n",
      "\n",
      "        [[ 0.9921875,  0.9921875,  0.9921875],\n",
      "         [ 0.9921875,  0.9921875,  0.9921875],\n",
      "         [ 0.7109375,  0.6953125,  0.6875   ],\n",
      "         ...,\n",
      "         [ 0.78125  ,  0.8203125,  0.8203125],\n",
      "         [ 0.6171875,  0.671875 ,  0.671875 ],\n",
      "         [ 0.65625  ,  0.703125 ,  0.703125 ]],\n",
      "\n",
      "        [[ 0.734375 ,  0.7109375,  0.703125 ],\n",
      "         [ 0.6640625,  0.6328125,  0.6171875],\n",
      "         [ 0.5390625,  0.4609375,  0.4375   ],\n",
      "         ...,\n",
      "         [ 0.421875 ,  0.5078125,  0.4921875],\n",
      "         [ 0.359375 ,  0.453125 ,  0.4375   ],\n",
      "         [ 0.3515625,  0.4375   ,  0.4140625]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.421875 , -0.3515625, -0.453125 ],\n",
      "         [-0.484375 , -0.3984375, -0.484375 ],\n",
      "         [-0.5234375, -0.46875  , -0.5234375],\n",
      "         ...,\n",
      "         [-0.34375  , -0.1640625, -0.3984375],\n",
      "         [-0.3515625, -0.1953125, -0.40625  ],\n",
      "         [-0.3828125, -0.2109375, -0.421875 ]],\n",
      "\n",
      "        [[-0.5078125, -0.390625 , -0.515625 ],\n",
      "         [-0.53125  , -0.3671875, -0.5078125],\n",
      "         [-0.484375 , -0.34375  , -0.4609375],\n",
      "         ...,\n",
      "         [-0.3203125, -0.15625  , -0.40625  ],\n",
      "         [-0.3046875, -0.1484375, -0.359375 ],\n",
      "         [-0.3046875, -0.109375 , -0.3125   ]],\n",
      "\n",
      "        [[-0.4453125, -0.3046875, -0.453125 ],\n",
      "         [-0.4921875, -0.3671875, -0.4921875],\n",
      "         [-0.5625   , -0.421875 , -0.5625   ],\n",
      "         ...,\n",
      "         [-0.4140625, -0.25     , -0.484375 ],\n",
      "         [-0.359375 , -0.21875  , -0.421875 ],\n",
      "         [-0.3515625, -0.1953125, -0.34375  ]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[-0.4765625, -0.3671875, -0.484375 ],\n",
      "         [-0.3984375, -0.3125   , -0.4296875],\n",
      "         [-0.46875  , -0.3828125, -0.421875 ],\n",
      "         ...,\n",
      "         [-0.265625 , -0.1796875, -0.1484375],\n",
      "         [ 0.4375   ,  0.5078125,  0.5234375],\n",
      "         [ 0.7578125,  0.796875 ,  0.765625 ]],\n",
      "\n",
      "        [[-0.453125 , -0.3515625, -0.4453125],\n",
      "         [-0.4140625, -0.328125 , -0.4296875],\n",
      "         [-0.453125 , -0.3828125, -0.40625  ],\n",
      "         ...,\n",
      "         [-0.3828125, -0.296875 , -0.265625 ],\n",
      "         [-0.015625 ,  0.0546875,  0.0859375],\n",
      "         [ 0.625    ,  0.6484375,  0.6796875]],\n",
      "\n",
      "        [[-0.453125 , -0.3515625, -0.40625  ],\n",
      "         [-0.40625  , -0.3203125, -0.390625 ],\n",
      "         [-0.46875  , -0.40625  , -0.4140625],\n",
      "         ...,\n",
      "         [-0.4296875, -0.3515625, -0.328125 ],\n",
      "         [-0.3671875, -0.3046875, -0.2578125],\n",
      "         [ 0.140625 ,  0.1640625,  0.25     ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.59375  ,  0.609375 ,  0.421875 ],\n",
      "         [ 0.546875 ,  0.5546875,  0.421875 ],\n",
      "         [ 0.6015625,  0.6171875,  0.453125 ],\n",
      "         ...,\n",
      "         [ 0.234375 ,  0.21875  ,  0.0625   ],\n",
      "         [ 0.1640625,  0.140625 , -0.0078125],\n",
      "         [ 0.15625  ,  0.1015625, -0.0234375]],\n",
      "\n",
      "        [[ 0.5234375,  0.5390625,  0.3671875],\n",
      "         [ 0.515625 ,  0.53125  ,  0.40625  ],\n",
      "         [ 0.34375  ,  0.359375 ,  0.21875  ],\n",
      "         ...,\n",
      "         [ 0.3046875,  0.2578125,  0.1171875],\n",
      "         [ 0.2421875,  0.1953125,  0.0546875],\n",
      "         [ 0.28125  ,  0.2265625,  0.1015625]],\n",
      "\n",
      "        [[ 0.2265625,  0.25     ,  0.078125 ],\n",
      "         [-0.0546875, -0.0390625, -0.140625 ],\n",
      "         [-0.25     , -0.2265625, -0.34375  ],\n",
      "         ...,\n",
      "         [ 0.46875  ,  0.40625  ,  0.265625 ],\n",
      "         [ 0.3984375,  0.3359375,  0.203125 ],\n",
      "         [ 0.375    ,  0.3203125,  0.1953125]]],\n",
      "\n",
      "\n",
      "       [[[-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         ...,\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ]],\n",
      "\n",
      "        [[-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         ...,\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ]],\n",
      "\n",
      "        [[-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         ...,\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         ...,\n",
      "         [-0.9921875, -1.       , -0.9765625],\n",
      "         [-1.       , -1.       , -0.9921875],\n",
      "         [-1.       , -1.       , -0.984375 ]],\n",
      "\n",
      "        [[-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         ...,\n",
      "         [-1.       , -0.9921875, -0.96875  ],\n",
      "         [-1.       , -0.9921875, -0.96875  ],\n",
      "         [-1.       , -1.       , -0.9765625]],\n",
      "\n",
      "        [[-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         [-1.       , -1.       , -1.       ],\n",
      "         ...,\n",
      "         [-1.       , -0.9921875, -0.9375   ],\n",
      "         [-1.       , -0.9921875, -0.9296875],\n",
      "         [-1.       , -1.       , -0.9609375]]],\n",
      "\n",
      "\n",
      "       [[[-0.21875  , -0.1484375, -0.34375  ],\n",
      "         [-0.046875 , -0.015625 , -0.1484375],\n",
      "         [-0.3203125, -0.3203125, -0.40625  ],\n",
      "         ...,\n",
      "         [-0.4140625, -0.390625 , -0.4609375],\n",
      "         [-0.453125 , -0.4296875, -0.5      ],\n",
      "         [-0.453125 , -0.4296875, -0.5      ]],\n",
      "\n",
      "        [[-0.328125 , -0.2265625, -0.4453125],\n",
      "         [-0.40625  , -0.34375  , -0.484375 ],\n",
      "         [-0.53125  , -0.5      , -0.578125 ],\n",
      "         ...,\n",
      "         [-0.390625 , -0.3671875, -0.4375   ],\n",
      "         [-0.40625  , -0.3828125, -0.453125 ],\n",
      "         [-0.359375 , -0.3359375, -0.40625  ]],\n",
      "\n",
      "        [[-0.34375  , -0.21875  , -0.4296875],\n",
      "         [-0.3515625, -0.2578125, -0.4140625],\n",
      "         [-0.3984375, -0.328125 , -0.4375   ],\n",
      "         ...,\n",
      "         [-0.421875 , -0.3984375, -0.46875  ],\n",
      "         [-0.4296875, -0.40625  , -0.4765625],\n",
      "         [-0.3515625, -0.328125 , -0.3984375]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5      , -0.3984375, -0.3828125],\n",
      "         [-0.046875 ,  0.0390625,  0.125    ],\n",
      "         [ 0.046875 ,  0.1328125,  0.21875  ],\n",
      "         ...,\n",
      "         [-0.1796875, -0.0390625, -0.0625   ],\n",
      "         [-0.34375  , -0.1953125, -0.2890625],\n",
      "         [-0.53125  , -0.3125   , -0.5078125]],\n",
      "\n",
      "        [[-0.65625  , -0.6015625, -0.5625   ],\n",
      "         [-0.390625 , -0.34375  , -0.265625 ],\n",
      "         [-0.25     , -0.2109375, -0.140625 ],\n",
      "         ...,\n",
      "         [ 0.1484375,  0.2890625,  0.390625 ],\n",
      "         [-0.015625 ,  0.1171875,  0.1640625],\n",
      "         [-0.265625 , -0.0859375, -0.1171875]],\n",
      "\n",
      "        [[-0.71875  , -0.7265625, -0.6953125],\n",
      "         [-0.5390625, -0.546875 , -0.484375 ],\n",
      "         [-0.359375 , -0.3671875, -0.3359375],\n",
      "         ...,\n",
      "         [ 0.046875 ,  0.1953125,  0.25     ],\n",
      "         [ 0.078125 ,  0.1875   ,  0.234375 ],\n",
      "         [ 0.015625 ,  0.15625  ,  0.171875 ]]]], dtype=float32)>, <tf.Tensor: shape=(32, 10), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prizl\\Documents\\GitHub\\IANNwTFGr10\\Homework\\06\\Gr10HW06.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training(network)\n",
      "\u001b[1;32mc:\\Users\\prizl\\Documents\\GitHub\\IANNwTFGr10\\Homework\\06\\Gr10HW06.ipynb Cell 31\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(network)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06.ipynb#Y130sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_dataset, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06.ipynb#Y130sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(data)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06.ipynb#Y130sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     metrics \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06.ipynb#Y130sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06.ipynb#Y130sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m network\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "training(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Restaurant Review - Visualization\n",
    "\n",
    "**Run the cell below to see how your model performed with the selected menu options.**\n",
    "\n",
    "*Note - this section fulfills part 2.4 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "fig = plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(val_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2,line3,line4),(\"Training Loss\",\"Test Loss\",\"Training Accuracy\",\"Test Accuracy\"))\n",
    "fig.savefig(\"CNN Performance CIFAR-10\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overfitting Report\n",
    "\n",
    "Our original model overfit the data, as evidenced by the plateau in validation (testing) performance versus training. This discrepancy indicates it began to use features in the training data which did not help it generalize when it saw the new data in the test batch.\n",
    "\n",
    "## Optimization Report\n",
    "\n",
    "We attempted the following optimization techniques and report on our reasoning for them and the results we obtained with them below:\n",
    "\n",
    "Data Augmentation:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "L1 Regularization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "L2 Regularization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Dropout:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Batch Normalization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Early Stopping :\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Old Plotting Code when Fit and Compile Were Used - Piper didn't want to delete this without permission. :)\n",
    "\n",
    "# plotting\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(6, 1, figsize=(8, 10))\n",
    "\n",
    "ax0.set_title(\"original\")\n",
    "ax0.plot(original.history[\"total_frobenius_norm\"]/np.max(original.history[\"total_frobenius_norm\"]) * np.max(original.history[\"val_loss\"]))\n",
    "ax0.plot(original.history[\"val_loss\"])\n",
    "ax0.plot(original.history[\"loss\"])\n",
    "ax0.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax0.savefig(\"convnet_original\")\n",
    "# ax0.show()\n",
    "\n",
    "ax1.set_title(\"L1\")\n",
    "ax1.plot(lassoReg.history[\"total_frobenius_norm\"]/np.max(lassoReg.history[\"total_frobenius_norm\"]) * np.max(lassoReg.history[\"val_loss\"]))\n",
    "ax1.plot(lassoReg.history[\"val_loss\"])\n",
    "ax1.plot(lassoReg.history[\"loss\"])\n",
    "# ax1.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax1.savefig(\"convnet_L1\")\n",
    "# ax1.show()\n",
    "\n",
    "ax2.set_title(\"L2\")\n",
    "ax2.plot(ridgeReg.history[\"total_frobenius_norm\"]/np.max(ridgeReg.history[\"total_frobenius_norm\"]) * np.max(ridgeReg.history[\"val_loss\"]))\n",
    "ax2.plot(ridgeReg.history[\"val_loss\"])\n",
    "ax2.plot(ridgeReg.history[\"loss\"])\n",
    "# ax2.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax2.savefig(\"convnet_L2\")\n",
    "# ax2.show()\n",
    "\n",
    "ax3.set_title(\"L2\")\n",
    "ax3.plot(augment.history[\"total_frobenius_norm\"]/np.max(augment.history[\"total_frobenius_norm\"]) * np.max(augment.history[\"val_loss\"]))\n",
    "ax3.plot(augment.history[\"val_loss\"])\n",
    "ax3.plot(augment.history[\"loss\"])\n",
    "# ax3.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax3.savefig(\"convnet_augment\")\n",
    "# ax3.show()\n",
    "\n",
    "ax4.set_title(\"Dropout\")\n",
    "ax4.plot(dropout.history[\"total_frobenius_norm\"]/np.max(dropout.history[\"total_frobenius_norm\"]) * np.max(dropout.history[\"val_loss\"]))\n",
    "ax4.plot(dropout.history[\"val_loss\"])\n",
    "ax4.plot(dropout.history[\"loss\"])\n",
    "# ax4.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax4.savefig(\"convnet_dropout\")\n",
    "# ax4.show()\n",
    "\n",
    "ax5.set_title(\"Batch Norm\")\n",
    "ax5.plot(batchnorm.history[\"total_frobenius_norm\"]/np.max(batchnorm.history[\"total_frobenius_norm\"]) * np.max(batchnorm.history[\"val_loss\"]))\n",
    "ax5.plot(batchnorm.history[\"val_loss\"])\n",
    "ax5.plot(batchnorm.history[\"loss\"])\n",
    "# ax5.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax5.savefig(\"convnet_batchnorm\")\n",
    "# ax5.show()\n",
    "\n",
    "ax5.set_xlabel(\"Epochs\")\n",
    "# ax5.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "fig.savefig(\"5 optimization methods comparison (e=15)\")\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
