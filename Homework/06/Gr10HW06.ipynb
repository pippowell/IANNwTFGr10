{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Homework 6\n",
    " ## Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 1 - Reviews\n",
    "\n",
    "We review the homeworks for groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 2 - CIFAR-10 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## READ ME\n",
    "\n",
    "This notebook does not work like a standard notebook. It is instead dynamic, allowing you to decide what optimization techniques (or combinations of them) you wish to apply. Follow the instructions in bold throughout this sheet to ensure you do not miss any required cells and that you fully understand your options (the menu) for each customizable section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run all of the following cells until you encounter a menu option instruction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tqdm\n",
    "import keras_cv # install keras_cv with pip for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "        # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_aug(dataset):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])\n",
    "\n",
    "    dataset = dataset.map(lambda x, y : (augmentation_model(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load ('cifar10', split =['train', 'test'], as_supervised = True, with_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Appetizers - Dataset Options\n",
    "\n",
    "**You have two options for this part of the menu:**\n",
    "\n",
    "**- Option 1: standard preprocessing for the CIFAR-10 dataset**\n",
    "**- Option 2: preprocessing w/ data augmentation for this dataset**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "**If you would like to see a readout and/or examples from the raw dataset, run the cell immediately following this message.**\n",
    "\n",
    "*Note - this section and the two preprocessing functions defined above fulfill part 2.1 of the assignment.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"ds_info: \\n\", ds_info)\n",
    "visualize a sample of the dataset\n",
    "tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 - Just the Standard Please\n",
    "\n",
    "train_dataset = preprocess(train_ds)\n",
    "test_dataset = preprocess(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - Augment My Meal\n",
    "\n",
    "train_dataset = preprocess_aug(train_ds)\n",
    "test_dataset = preprocess_aug(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the following cell regardless. It defines the basic CNN model we will be using. Don't worry, you'll get to play around with optimization options for it later. :)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    def __init__(self, L1_reg=0, L2_reg=0, dropout_rate=0, batch_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        if L2_reg >= 0: # L2 is the default\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L2_reg)\n",
    "        elif L1_reg > 0: \n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L1_reg)\n",
    "        else:\n",
    "            None\n",
    "\n",
    "        if self.dropout_rate:\n",
    "            self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "        # input 32x32x3 with 3 as the color channels\n",
    "        if batch_norm:\n",
    "            self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                               tf.keras.layers.BatchNormalization(),\n",
    "                                tf.keras.layers.GlobalAvgPool2D(),\n",
    "                                tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "\n",
    "        else:\n",
    "            self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), # 32x32x32\n",
    "                                tf.keras.layers.GlobalAvgPool2D(),\n",
    "                                tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "\n",
    "        self.metrics_list = [ \n",
    "                    tf.keras.metrics.Mean(name=\"total_frobenius_norm\"),\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                    ]\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.dropout_rate:\n",
    "                x = self.dropout_layer(x, training)\n",
    "\n",
    "        return self.layer_list[-1](x)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "\n",
    "        img, label = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(img, training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "            # loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.metrics[0].update_state(self.compute_frobenius())\n",
    "        self.metrics[1].update_state(loss)\n",
    "        self.metrics[2].update_state(label, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "\n",
    "        img, label = data\n",
    "\n",
    "        prediction = self(img, training=False)\n",
    "        loss = self.loss_function(label, prediction)\n",
    "        # loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "\n",
    "        self.metrics[0].update_state(self.compute_frobenius())\n",
    "        self.metrics[1].update_state(loss)\n",
    "        self.metrics[2].update_state(label, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### First Course - In-Model Optimization Options\n",
    "\n",
    "**You have four options for this part of the menu:**\n",
    "\n",
    "**- Option 1: the basic model w/ no optimizations applied**\n",
    "**- Option 2: the basic model w/ L1 regularization**\n",
    "**- Option 3: the basic model w/ L2 regularization**\n",
    "**- Option 4: the basic model w/ dropout layers**\n",
    "**- Option 5: the basic model w/ batch normalization**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "*Note - this section and the model code above fulfill part 2.2 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 1 - Plane Jane\n",
    "\n",
    "network = ConvModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 2 - Lovin' L1\n",
    "\n",
    "network = ConvModel(L1_reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 3 - Lovin' L2\n",
    "\n",
    "network = ConvModel(L2_reg=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 4 - Drop It\n",
    "\n",
    "network = ConvModel(dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Option 5 - Nabbin' the Norm\n",
    "\n",
    "network = ConvModel(batch_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the next cell regardless. It sets up the logging and metrics we'll need for visualization later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"HW06\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_forb_norm = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "val_forb_norm = [] # Q. do we need both?\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the following cells regardless. They prepare the training options you will choose between later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def training(network):\n",
    "\n",
    "    '''\n",
    "    :param network: the network model to be trained\n",
    "    '''\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                train_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of metrics\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"total_frobenius_norm\":\n",
    "                val_forb_norm.append(value.numpy())\n",
    "            elif key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "            else:\n",
    "                None\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def training_es(network):\n",
    "\n",
    "    '''\n",
    "    :param network: the network model to be trained\n",
    "    '''\n",
    "\n",
    "    # define a counter for the number of epochs when the validation loss increased\n",
    "    lossincreasecount = 0\n",
    "\n",
    "    # define how many epochs you will run for at the start before caring whether validation loss is increasing\n",
    "    guaranteedepochs = 1\n",
    "\n",
    "    # define how many consecutive epochs you will tolerate the validation loss increasing for before halting training\n",
    "    tolerableincreaseepochs = 1\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # always run training for the guaranteed number of epochs defined above\n",
    "        if epoch < guaranteedepochs:\n",
    "            print('gauranteed epoch')\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "        # after the guaranteed number of epochs, start monitoring val loss to determine when training should stop\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # if you see the validation loss increasing for more than a set amount of consecutive epochs, terminate training\n",
    "            if val_losses[epoch]>val_losses[epoch-1] and lossincreasecount == tolerableincreaseepochs:\n",
    "                print('val loss still increasing beyond tolerated epoch number, halting training. Last loss was ' + str(val_losses[epoch]) + '.')\n",
    "                break\n",
    "\n",
    "            if val_losses[epoch]>val_losses[epoch-1] and lossincreasecount != tolerableincreaseepochs:\n",
    "                lossincreasecount=++1\n",
    "                print('val loss has increased for ' + str(lossincreasecount) + ' epochs. Last loss was ' + str(val_losses[epoch]) + '.')\n",
    "\n",
    "            elif val_losses[epoch]<=val_losses[epoch-1]:\n",
    "                print('val loss decreased or stayed the same, continuing')\n",
    "                lossincreasecount = 0\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second Course - Training Options\n",
    "\n",
    "**You have two options for this part of the menu:**\n",
    "\n",
    "**- Option 1: standard training**\n",
    "**- Option 2: training that implements early stopping**\n",
    "\n",
    "**Run the cell corresponding to your choice and then proceed with the sheet.**\n",
    "\n",
    "**If you would like to monitor the training in the TensorBoard, run the cell immediately following this message.**\n",
    "\n",
    "*Note - this section fulfills part 2.3 of the assignment.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare the tensorboard ahead of training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Option 1 - The Usual \n",
    "\n",
    "training(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gauranteed epoch\n",
      "Epoch: 0, optimizer: <keras.optimizers.optimizer_v2.adam.Adam object at 0x00000230A8845CA0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1053/1563 [00:55<00:26, 19.55it/s]"
     ]
    }
   ],
   "source": [
    "# Option 2 - The Early Bird \n",
    "\n",
    "training_es(network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Restaurant Review - Visualization\n",
    "\n",
    "**Run the cell below to see how the model performed with the selected menu options.**\n",
    "\n",
    "*Note - this section fulfills part 2.4 of the assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "fig = plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(val_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2,line3,line4),(\"Training Loss\",\"Test Loss\",\"Training Accuracy\",\"Test Accuracy\"))\n",
    "fig.savefig(\"CNN Performance CIFAR-10\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overfitting Report\n",
    "\n",
    "Our original model overfit the data, as evidenced by the plateau in validation (testing) performance versus training. This discrepancy indicates it began to use features in the training data which did not help it generalize when it saw the new data in the test batch.\n",
    "\n",
    "## Optimization Report\n",
    "\n",
    "We attempted the following optimization techniques and report on our reasoning for them and the results we obtained with them below:\n",
    "\n",
    "Data Augmentation:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "L1 Regularization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "L2 Regularization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Dropout:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Batch Normalization:\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "Early Stopping :\n",
    "- *Why?*:\n",
    "- *Observations*:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}