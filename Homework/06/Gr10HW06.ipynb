{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Homework 5\n",
    " ## Group 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 1 - Reviews\n",
    "\n",
    "We review the homeworks for groups 15 and 32."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assignment 2 - CIFAR-10 Classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the necessary imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Prepare the Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors - we don't do this step for CNN, CNN layers expect standard image format input\n",
    "    # dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_info: \n",
      " tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='C:\\\\Users\\\\powel\\\\tensorflow_datasets\\\\cifar10\\\\3.0.2',\n",
      "    file_format=tfrecord,\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load ('cifar10', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "print(\"ds_info: \\n\", ds_info)\n",
    "\n",
    "# visualize a sample of the dataset\n",
    "# tfds.show_examples(train_ds, ds_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess(train_ds)\n",
    "test_dataset = preprocess(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Optimize the CNN Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Original Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# straight from the lecture\n",
    "class BasicConv(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BasicConv, self).__init__()\n",
    "\n",
    "        # input 32x32x3 with 3 as the color channels\n",
    "        self.convlayer1 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # after this: 32x32x24\n",
    "        self.convlayer2 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "        self.convlayer3 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "\n",
    "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2) # 16x16x24\n",
    "\n",
    "        #self.normlayer = tf.keras.layers.Normalization(axis=-1,mean=None,invert=False)\n",
    "\n",
    "        self.convlayer4 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x48\n",
    "        self.convlayer5 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x48\n",
    "        self.convlayer6 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "\n",
    "        self.global_pool = tf.keras.layers.GlobalAvgPool2D() # 1x1x48\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        self.metrics_list = [\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"acc\"), # only for subtask 0, not for subtask 1\n",
    "                    ]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = self.convlayer3(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.convlayer4(x)\n",
    "        x = self.convlayer5(x)\n",
    "        x = self.convlayer6(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input):\n",
    "        img, label = input\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(img, training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "\n",
    "        img, label = input\n",
    "\n",
    "        prediction = self(img, training=False)\n",
    "        loss = self.loss_function(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy for Optimization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# straight from the lecture\n",
    "class OptConv(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(OptConv, self).__init__()\n",
    "\n",
    "        # input 32x32x3 with 3 as the color channels\n",
    "        self.convlayer1 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # after this: 32x32x24\n",
    "        self.convlayer2 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "        self.convlayer3 = tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "\n",
    "        self.pooling = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2) # 16x16x24\n",
    "\n",
    "        #self.normlayer = tf.keras.layers.Normalization(axis=-1,mean=None,invert=False)\n",
    "\n",
    "        self.convlayer4 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x48\n",
    "        self.convlayer5 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 16x16x48\n",
    "        self.convlayer6 = tf.keras.layers.Conv2D(filters=72, kernel_size=3, padding='same', activation='relu') # 32x32x24\n",
    "\n",
    "        self.global_pool = tf.keras.layers.GlobalAvgPool2D() # 1x1x48\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        self.metrics_list = [\n",
    "                    tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                    tf.keras.metrics.BinaryAccuracy(name=\"acc\"), # only for subtask 0, not for subtask 1\n",
    "                    ]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.convlayer1(x)\n",
    "        x = self.convlayer2(x)\n",
    "        x = self.convlayer3(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.convlayer4(x)\n",
    "        x = self.convlayer5(x)\n",
    "        x = self.convlayer6(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, input):\n",
    "        img, label = input\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(img, training=True)\n",
    "            loss = self.loss_function(label, prediction)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "\n",
    "        img, label = input\n",
    "\n",
    "        prediction = self(img, training=False)\n",
    "        loss = self.loss_function(label, prediction) # + tf.reduce_sum(self.losses)\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(label, prediction)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Original Training Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "\n",
    "def training():\n",
    "\n",
    "    # Select the model to use - the original or the modified one for optimization\n",
    "    network = BasicConv()\n",
    "    #network = OptConv()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Loop w/ Early Stopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "\n",
    "def training():\n",
    "\n",
    "    # define the number of epochs you want to definitely train for (i.e. ones where you don't worry if the validation loss is not decreasing)\n",
    "    deftrain = 1\n",
    "\n",
    "    # Select the model to use - the original or the modified one for optimization\n",
    "    network = BasicConv()\n",
    "    #network = OptConv()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        if epoch < deftrain:\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "            print('guaranteed epoch')\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "            if val_losses[epoch]>=val_losses[epoch-1]:\n",
    "                print('stopping early, validation loss not decreasing')\n",
    "                break\n",
    "            else:\n",
    "                print('continuing, validation loss decreasing')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prepare the tensorboard ahead of training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, optimizer: <keras.optimizers.optimizer_v2.adam.Adam object at 0x00000267A08DA970>\n",
      "guaranteed epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [10:11<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 1.6202869415283203', 'train_acc: 0.9073879718780518']\n",
      "['val_loss: 1.2967520952224731', 'val_acc: 0.9196199774742126']\n",
      "Epoch: 1, optimizer: <keras.optimizers.optimizer_v2.adam.Adam object at 0x00000267A08DA970>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [08:49<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 1.156718134880066', 'train_acc: 0.9276720285415649']\n",
      "['val_loss: 1.0678272247314453', 'val_acc: 0.9338799715042114']\n",
      "continuing, validation loss decreasing\n",
      "Epoch: 2, optimizer: <keras.optimizers.optimizer_v2.adam.Adam object at 0x00000267A08DA970>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1563/1563 [08:57<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_loss: 0.9702072143554688', 'train_acc: 0.9382579922676086']\n",
      "['val_loss: 0.9587374329566956', 'val_acc: 0.9395400285720825']\n",
      "stopping early, validation loss not decreasing\n"
     ]
    }
   ],
   "source": [
    "training()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 - Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "fig = plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(val_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2,line3,line4),(\"Training Loss\",\"Test Loss\",\"Training Accuracy\",\"Test Accuracy\"))\n",
    "fig.savefig(\"CNN Performance CIFAR-10\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Report re Overfitting\n",
    "\n",
    "Our original model overfit the data, as evidenced by the plateau in validation (testing) performance versus training. This discrepancy indicates it began to use features in the training data which did not help it generalize when it saw the new data in the test batch.\n",
    "\n",
    "## Report re Optimization\n",
    "\n",
    "We attempted the following optimization techniques and report on our reasoning for them and the results we obtained with them below:\n",
    "\n",
    "1. L1 Regularization\n",
    "\n",
    "2. L2 Regularization\n",
    "\n",
    "3. Dropout\n",
    "\n",
    "4. Normalizing Input Data\n",
    "\n",
    "5. Early stopping\n",
    "\n",
    "Other ideas - normalizing inside network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}