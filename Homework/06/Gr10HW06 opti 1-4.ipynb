{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Homework 5 - opti 1, 2, 3, 4 \n",
    " ## Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 1 - Reviews\n",
    "\n",
    "We review the homeworks for groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 2 - CIFAR-10 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prizl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# the necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tqdm\n",
    "import keras_cv # install keras_cv with pip for data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors - we don't do this step for CNN, CNN layers expect standard image format input\n",
    "    # dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load ('cifar10', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "# print(\"ds_info: \\n\", ds_info)\n",
    "\n",
    "# visualize a sample of the dataset\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess(train_ds)\n",
    "test_dataset = preprocess(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    def __init__(self, L2_reg=0, dropout_rate=0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        kernel_regularizer=tf.keras.regularizers.L2(L2_reg) if L2_reg else None\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if self.dropout_rate:\n",
    "            self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "            \n",
    "        self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "            tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        if batch_norm:\n",
    "            \n",
    "                self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                   #tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        # metrics to update\n",
    "        self.frobenius_metric = tf.keras.metrics.Mean(name=\"total_frobenius_norm\")\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.dropout_rate:\n",
    "                x = self.dropout_layer(x, training)\n",
    "        \n",
    "        return self.layer_list[-1](x)\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(x, training=True)\n",
    "            loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.accuracy_metric.update_state(target, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, target = data\n",
    "        prediction = self(x, training=False)\n",
    "        loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        \n",
    "        self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.accuracy_metric.update_state(target, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the model without any extra regularization (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "model = ConvModel()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "original = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(original.history[\"total_frobenius_norm\"]/np.max(original.history[\"total_frobenius_norm\"]) * np.max(original.history[\"val_loss\"]))\n",
    "plt.plot(original.history[\"val_loss\"])\n",
    "plt.plot(original.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_no_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the same model with L2 regularization (ridgeReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel(L2_reg=0.001)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "ridgeReg = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ridgeReg.history[\"total_frobenius_norm\"]/np.max(ridgeReg.history[\"total_frobenius_norm\"]) * np.max(ridgeReg.history[\"val_loss\"]))\n",
    "plt.plot(ridgeReg.history[\"val_loss\"])\n",
    "plt.plot(ridgeReg.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_l2_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the same model with only data augmentation --> not done (aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])\n",
    "\n",
    "train_ds, val_ds = get_cifar10(32, augmentation=augmentation_model) # --> AUGMENTATION NEEDS TO BE SPECIFIED\n",
    "\n",
    "model = ConvModel()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "aug = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aug.history[\"total_frobenius_norm\"]/np.max(aug.history[\"total_frobenius_norm\"]) * np.max(aug.history[\"val_loss\"]))\n",
    "plt.plot(aug.history[\"val_loss\"])\n",
    "plt.plot(aug.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_augment_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the same model with only dropout between layers (dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel(L2_reg=0, dropout_rate=0.5)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "dropout = model.fit(train_ds, validation_data=val_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dropout.history[\"total_frobenius_norm\"]/np.max(dropout.history[\"total_frobenius_norm\"]) * np.max(dropout.history[\"val_loss\"]))\n",
    "plt.plot(dropout.history[\"val_loss\"])\n",
    "plt.plot(dropout.history[\"loss\"])\n",
    "plt.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "plt.savefig(\"convnet_dropout_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Original Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "\n",
    "def training():\n",
    "\n",
    "    # Select the model to use - the original or the modified one for optimization\n",
    "    # network = BasicConv()\n",
    "    network = ConvModel()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training Loop w/ Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the training loop\n",
    "val_losses.append(100)\n",
    "val_losses.append(50)\n",
    "\n",
    "def training():\n",
    "\n",
    "    # Select the model to use - the original or the modified one for optimization\n",
    "    network = BasicConv()\n",
    "    #network = OptConv()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        while val_losses[epoch]<val_losses[epoch-1]:\n",
    "            print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "            for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "                metrics = network.train_step(data)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    train_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    train_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset metrics for next round\n",
    "            network.reset_metrics()\n",
    "\n",
    "            # Testing\n",
    "            for data in test_dataset:\n",
    "                metrics = network.test_step(data)\n",
    "\n",
    "                # log the accs and losses\n",
    "                with val_summary_writer.as_default():\n",
    "                    for metric in network.metrics:\n",
    "                        tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "            # print the end acc and loss\n",
    "            print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "            # make a list of losses and accuracies\n",
    "            for (key, value) in metrics.items():\n",
    "                if key == \"loss\":\n",
    "                    val_losses.append(value.numpy())\n",
    "                elif key == \"acc\":\n",
    "                    val_accuracies.append(value.numpy())\n",
    "\n",
    "            # reset all metrics\n",
    "            network.reset_metrics()\n",
    "\n",
    "val_losses.pop(0)\n",
    "val_losses.pop(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare the tensorboard ahead of training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "fig = plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(val_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2,line3,line4),(\"Training Loss\",\"Test Loss\",\"Training Accuracy\",\"Test Accuracy\"))\n",
    "fig.savefig(\"CNN Performance CIFAR-10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Report re Overfitting\n",
    "\n",
    "Our original model overfit the data, as evidenced by the plateau in validation (testing) performance versus training. This discrepancy indicates it began to use features in the training data which did not help it generalize when it saw the new data in the test batch.\n",
    "\n",
    "## Report re Optimization\n",
    "\n",
    "We attempted the following optimization techniques and report on our reasoning for them and the results we obtained with them below:\n",
    "\n",
    "1. L1 Regularization\n",
    "\n",
    "2. L2 Regularization\n",
    "\n",
    "3. Dropout\n",
    "\n",
    "4. Normalizing Input Data\n",
    "\n",
    "5. Early stopping\n",
    "\n",
    "Other ideas - normalizing inside network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
