{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " # Homework 5 - opti 0 (original), 1 (L1), 2 (L2), 3 (data augmentation), 4 (dropout), 5 (batchnorm)\n",
    " ## Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 1 - Reviews\n",
    "\n",
    "We review the homeworks for groups 15 and 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Assignment 2 - CIFAR-10 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prizl\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# the necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as datetime\n",
    "import tqdm\n",
    "import keras_cv # install keras_cv with pip for data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors - we don't do this step for CNN, CNN layers expect standard image format input\n",
    "    # dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_aug(dataset, augmentation):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "\n",
    "    # flatten the images into vectors - we don't do this step for CNN, CNN layers expect standard image format input\n",
    "    # dataset = dataset.map(lambda img, target: (tf.reshape(img, (-1,)), target))\n",
    "\n",
    "    # input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img / 128.) - 1., target))\n",
    "\n",
    "    # create one-hot targets with depth 10 since cifar 10 has 10 classes\n",
    "    dataset = dataset.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "    # cache\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle, batch, prefetch\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.batch(32)\n",
    "\n",
    "    if augmentation:\n",
    "        dataset = dataset.map(lambda x, y : (augmentation_model(x), y),num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "    # return preprocessed dataset\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_ds, test_ds), ds_info = tfds.load ('cifar10', split =['train', 'test'], as_supervised = True, with_info = True)\n",
    "\n",
    "# print(\"ds_info: \\n\", ds_info)\n",
    "\n",
    "# visualize a sample of the dataset\n",
    "# tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess(train_ds)\n",
    "train_dataset_aug = preprocess_aug(train_ds)\n",
    "test_dataset = preprocess(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvModel(tf.keras.Model):\n",
    "    def __init__(self, L1_reg=0, L2_reg=0, dropout_rate=0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if L2_reg >= 0: # L2 is the default\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L2_reg)\n",
    "        elif L1_reg > 0:\n",
    "            kernel_regularizer=tf.keras.regularizers.L2(L1_reg)\n",
    "        else:\n",
    "            None\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        if self.dropout_rate:\n",
    "            self.dropout_layer = tf.keras.layers.Dropout(dropout_rate)\n",
    "            \n",
    "        self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "            tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        \n",
    "        if batch_norm:    \n",
    "                self.layer_list = [tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer), \n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3,activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Conv2D(32, 3, activation=\"relu\", kernel_regularizer=kernel_regularizer),\n",
    "                                   tf.keras.layers.BatchNormalization(),\n",
    "                                    tf.keras.layers.Flatten(),\n",
    "                                    tf.keras.layers.BatchNormalization(), # why was this commented?\n",
    "                                    tf.keras.layers.Dense(10, activation=\"softmax\", kernel_regularizer=kernel_regularizer)]\n",
    "        # metrics to update\n",
    "        self.frobenius_metric = tf.keras.metrics.Mean(name=\"total_frobenius_norm\")\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layer_list[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.dropout_rate:\n",
    "                x = self.dropout_layer(x, training)\n",
    "        \n",
    "        return self.layer_list[-1](x)\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "            \n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        x, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self(x, training=True)\n",
    "            loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.accuracy_metric.update_state(target, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        x, target = data\n",
    "        prediction = self(x, training=False)\n",
    "        loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n",
    "        \n",
    "        self.frobenius_metric.update_state(self.compute_frobenius())\n",
    "        self.loss_metric.update_state(loss)\n",
    "        self.accuracy_metric.update_state(target, prediction)\n",
    "        \n",
    "        return {metric.name: metric.result() for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"HW06\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(x: int):\n",
    "\n",
    "    if x == 0: # original\n",
    "        network = ConvModel()\n",
    "    elif x == 1: # L1\n",
    "        network = ConvModel(L1_reg=0.001)\n",
    "    elif x == 2: # L2\n",
    "        network = ConvModel(L2_reg=0.01)\n",
    "    elif x == 3: # data aug\n",
    "        augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])\n",
    "        network = ConvModel()\n",
    "    elif x == 4: # dropout\n",
    "        network = ConvModel(dropout_rate=0.5)\n",
    "    elif x == 5: # batchnorm\n",
    "        network = ConvModel(batch_norm=True)\n",
    "    else:\n",
    "        print(f\"Choose a number between 0 - 5.\")\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, optimizer: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1563 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\prizl\\AppData\\Local\\Temp\\ipykernel_8924\\706703681.py\", line 63, in train_step  *\n        loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n\n    TypeError: 'NoneType' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prizl\\Documents\\GitHub\\IANNwTFGr10\\Homework\\06\\Gr10HW06 opti 1-5.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06%20opti%201-5.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\prizl\\Documents\\GitHub\\IANNwTFGr10\\Homework\\06\\Gr10HW06 opti 1-5.ipynb Cell 15\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06%20opti%201-5.ipynb#X55sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, optimizer: \u001b[39m\u001b[39m{\u001b[39;00mnetwork\u001b[39m.\u001b[39moptimizer\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06%20opti%201-5.ipynb#X55sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(train_dataset, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06%20opti%201-5.ipynb#X55sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     metrics \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06%20opti%201-5.ipynb#X55sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prizl/Documents/GitHub/IANNwTFGr10/Homework/06/Gr10HW06%20opti%201-5.ipynb#X55sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m network\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev_ahmod2.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m     12\u001b[0m     prediction \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), (ag__\u001b[39m.\u001b[39mld(x),), \u001b[39mdict\u001b[39m(training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), fscope)\n\u001b[1;32m---> 13\u001b[0m     loss \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcompiled_loss, (ag__\u001b[39m.\u001b[39;49mld(target), ag__\u001b[39m.\u001b[39;49mld(prediction)), \u001b[39mdict\u001b[39;49m(regularization_losses\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mlosses), fscope)\n\u001b[0;32m     14\u001b[0m gradients \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tape)\u001b[39m.\u001b[39mgradient, (ag__\u001b[39m.\u001b[39mld(loss), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mapply_gradients, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mzip\u001b[39m), (ag__\u001b[39m.\u001b[39mld(gradients), ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtrainable_variables), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\prizl\\AppData\\Local\\Temp\\ipykernel_8924\\706703681.py\", line 63, in train_step  *\n        loss = self.compiled_loss(target, prediction, regularization_losses=self.losses)\n\n    TypeError: 'NoneType' object is not callable\n"
     ]
    }
   ],
   "source": [
    "training(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Training the model without any extra regularization (original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "original = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training the same model with L1 regularization (lassoReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel(L1_reg=0.001)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "lassoReg = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training the same model with L2 regularization (ridgeReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel(L2_reg=0.001)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "ridgeReg = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the same model with only data augmentation --> not done (aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_model = tf.keras.Sequential([keras_cv.layers.RandAugment(value_range=[0,1],magnitude=0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new training dataset for data augmentation since we defined two different preprocess methods: one with, the other without augmentation\n",
    "train_dataset_aug = preprocess_aug(train_ds, augmentation=augmentation_model)\n",
    "\n",
    "model = ConvModel()\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "augment = model.fit(train_dataset_aug, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the same model with only dropout between layers (dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel(dropout_rate=0.5)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "dropout = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the same model with only batch normalization (batchnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvModel(batch_norm=True)\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=\"adam\")\n",
    "batchnorm = model.fit(train_dataset, validation_data=test_dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, (ax0, ax1, ax2, ax3, ax4, ax5) = plt.subplots(6, 1, figsize=(8, 10))\n",
    "\n",
    "ax0.set_title(\"original\")\n",
    "ax0.plot(original.history[\"total_frobenius_norm\"]/np.max(original.history[\"total_frobenius_norm\"]) * np.max(original.history[\"val_loss\"]))\n",
    "ax0.plot(original.history[\"val_loss\"])\n",
    "ax0.plot(original.history[\"loss\"])\n",
    "ax0.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax0.savefig(\"convnet_original\")\n",
    "# ax0.show()\n",
    "\n",
    "ax1.set_title(\"L1\")\n",
    "ax1.plot(lassoReg.history[\"total_frobenius_norm\"]/np.max(lassoReg.history[\"total_frobenius_norm\"]) * np.max(lassoReg.history[\"val_loss\"]))\n",
    "ax1.plot(lassoReg.history[\"val_loss\"])\n",
    "ax1.plot(lassoReg.history[\"loss\"])\n",
    "# ax1.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax1.savefig(\"convnet_L1\")\n",
    "# ax1.show()\n",
    "\n",
    "ax2.set_title(\"L2\")\n",
    "ax2.plot(ridgeReg.history[\"total_frobenius_norm\"]/np.max(ridgeReg.history[\"total_frobenius_norm\"]) * np.max(ridgeReg.history[\"val_loss\"]))\n",
    "ax2.plot(ridgeReg.history[\"val_loss\"])\n",
    "ax2.plot(ridgeReg.history[\"loss\"])\n",
    "# ax2.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax2.savefig(\"convnet_L2\")\n",
    "# ax2.show()\n",
    "\n",
    "ax3.set_title(\"L2\")\n",
    "ax3.plot(augment.history[\"total_frobenius_norm\"]/np.max(augment.history[\"total_frobenius_norm\"]) * np.max(augment.history[\"val_loss\"]))\n",
    "ax3.plot(augment.history[\"val_loss\"])\n",
    "ax3.plot(augment.history[\"loss\"])\n",
    "# ax3.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax3.savefig(\"convnet_augment\")\n",
    "# ax3.show()\n",
    "\n",
    "ax4.set_title(\"Dropout\")\n",
    "ax4.plot(dropout.history[\"total_frobenius_norm\"]/np.max(dropout.history[\"total_frobenius_norm\"]) * np.max(dropout.history[\"val_loss\"]))\n",
    "ax4.plot(dropout.history[\"val_loss\"])\n",
    "ax4.plot(dropout.history[\"loss\"])\n",
    "# ax4.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax4.savefig(\"convnet_dropout\")\n",
    "# ax4.show()\n",
    "\n",
    "ax5.set_title(\"Batch Norm\")\n",
    "ax5.plot(batchnorm.history[\"total_frobenius_norm\"]/np.max(batchnorm.history[\"total_frobenius_norm\"]) * np.max(batchnorm.history[\"val_loss\"]))\n",
    "ax5.plot(batchnorm.history[\"val_loss\"])\n",
    "ax5.plot(batchnorm.history[\"loss\"])\n",
    "# ax5.legend(labels=[\"Total Frobenius Norm\", \"Validation Loss\", \"Loss\"])\n",
    "# ax5.savefig(\"convnet_batchnorm\")\n",
    "# ax5.show()\n",
    "\n",
    "ax5.set_xlabel(\"Epochs\")\n",
    "# ax5.set_ylabel(\"Loss\")\n",
    "\n",
    "plt.legend()\n",
    "fig.savefig(\"5 optimization methods comparison (e=15)\")\n",
    "\n",
    "fig.tight_layout(pad=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "revised until here (9.12 Wooki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initiate the logs and metrics\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "# Initiate epochs and learning rate as global variables\n",
    "epochs = 15\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Define arrays for saving values for later visualization\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define the training loop\n",
    "\n",
    "def training():\n",
    "\n",
    "    # Select the model to use - the original or the modified one for optimization\n",
    "    # network = BasicConv()\n",
    "    network = ConvModel()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch: {epoch}, optimizer: {network.optimizer}\")\n",
    "\n",
    "        for data in tqdm.tqdm(train_dataset, position=0, leave=True):\n",
    "            metrics = network.train_step(data)\n",
    "\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"train_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                train_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                train_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset metrics for next round\n",
    "        network.reset_metrics()\n",
    "\n",
    "        # Testing\n",
    "        for data in test_dataset:\n",
    "            metrics = network.test_step(data)\n",
    "\n",
    "            # log the accs and losses\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in network.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step = epoch)\n",
    "\n",
    "        # print the end acc and loss\n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # make a list of losses and accuracies\n",
    "        for (key, value) in metrics.items():\n",
    "            if key == \"loss\":\n",
    "                val_losses.append(value.numpy())\n",
    "            elif key == \"acc\":\n",
    "                val_accuracies.append(value.numpy())\n",
    "\n",
    "        # reset all metrics\n",
    "        network.reset_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prepare the tensorboard ahead of training\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 - Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "\n",
    "fig = plt.figure()\n",
    "line1, = plt.plot(train_losses)\n",
    "line2, = plt.plot(val_losses)\n",
    "line3, = plt.plot(train_accuracies)\n",
    "line4, = plt.plot(val_accuracies)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend((line1,line2,line3,line4),(\"Training Loss\",\"Test Loss\",\"Training Accuracy\",\"Test Accuracy\"))\n",
    "fig.savefig(\"CNN Performance CIFAR-10\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Report re Overfitting\n",
    "\n",
    "Our original model overfit the data, as evidenced by the plateau in validation (testing) performance versus training. This discrepancy indicates it began to use features in the training data which did not help it generalize when it saw the new data in the test batch.\n",
    "\n",
    "## Report re Optimization\n",
    "\n",
    "We attempted the following optimization techniques and report on our reasoning for them and the results we obtained with them below:\n",
    "\n",
    "0. original (no optimization applied)\n",
    "\n",
    "1. L2 Regularization\n",
    "\n",
    "2. Augmentation\n",
    "\n",
    "3. Dropout\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045caf6303e7720903cf179822b02fa228c285a06d63d48b635a33538dcbdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
